{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "958cb60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  # plots\n",
    "import numpy as np  # vectors and matrices\n",
    "import pandas as pd  # tables and data manipulations\n",
    "import seaborn as sns  # more plots\n",
    "sns.set()\n",
    "import warnings  \n",
    "warnings.filterwarnings('ignore')\n",
    "from itertools import product  # some useful functions\n",
    "import scipy.stats as scs\n",
    "from dateutil.relativedelta import relativedelta  # working with dates with style\n",
    "from scipy.optimize import minimize  # for function minimization\n",
    "from sklearn.model_selection import TimeSeriesSplit  # you have everything done for you \n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e70e0471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 156 entries, 0 to 155\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count  Dtype         \n",
      "---  ------  --------------  -----         \n",
      " 0   date    156 non-null    datetime64[ns]\n",
      " 1   SO2op1  156 non-null    float64       \n",
      " 2   SO2op2  156 non-null    float64       \n",
      " 3   T       156 non-null    float64       \n",
      " 4   SO2     156 non-null    float64       \n",
      " 5   NO2     156 non-null    float64       \n",
      "dtypes: datetime64[ns](1), float64(5)\n",
      "memory usage: 7.4 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df_G2 = pd.read_csv('G2.csv.xls', sep = ',') \n",
    "df_G2['date'] = df_G2['date'].map(lambda x: datetime.datetime.strptime(str(x), \"%Y-%m-%d %H:%M:%S\"))\n",
    "# print(df_G2)\n",
    "print(df_G2.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14774694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       SO2op1    SO2op2         T       NO2\n",
      "0    0.859881  0.031743  1.511389 -0.746816\n",
      "1   -0.695262 -0.663402 -0.929485  0.466760\n",
      "2    0.619063  0.874095  1.999564 -0.746816\n",
      "3    0.472447  0.162594 -0.199175 -0.746816\n",
      "4   -0.692746 -0.396248 -0.929485  0.466760\n",
      "..        ...       ...       ...       ...\n",
      "99  -0.698934 -0.717923 -0.683445  0.466760\n",
      "100 -1.531742 -1.884676 -0.441310  1.680336\n",
      "101  0.868926  1.124892  1.511389 -0.746816\n",
      "102  0.469336 -0.161807 -0.199175 -0.746816\n",
      "103 -0.693069 -0.529825 -0.929485  0.466760\n",
      "\n",
      "[104 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "y = df_G2.dropna()['SO2']\n",
    "X = df_G2.dropna().drop(['SO2'], axis=1)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.33)\n",
    "\n",
    "time_train = X_train.dropna()['date']\n",
    "X_train = X_train.drop(['date'], axis=1)\n",
    "time_valid = X_valid.dropna()['date']\n",
    "X_valid = X_valid.drop(['date'], axis=1)\n",
    "\n",
    "\n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train))\n",
    "X_valid_scaled = pd.DataFrame(scaler.transform(X_valid))\n",
    "list_X = X_train.columns.values.tolist()\n",
    "X_train_scaled.set_axis(list_X, axis = 'columns', inplace=True)\n",
    "X_valid_scaled.set_axis(list_X, axis = 'columns', inplace=True)\n",
    "\n",
    "\n",
    "print(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "975fc6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       SO2op1    SO2op2         T       NO2\n",
      "0    0.859881  0.031743  1.511389 -0.746816\n",
      "1   -0.695262 -0.663402 -0.929485  0.466760\n",
      "2    0.619063  0.874095  1.999564 -0.746816\n",
      "3    0.472447  0.162594 -0.199175 -0.746816\n",
      "4   -0.692746 -0.396248 -0.929485  0.466760\n",
      "..        ...       ...       ...       ...\n",
      "99  -0.698934 -0.717923 -0.683445  0.466760\n",
      "100 -1.531742 -1.884676 -0.441310  1.680336\n",
      "101  0.868926  1.124892  1.511389 -0.746816\n",
      "102  0.469336 -0.161807 -0.199175 -0.746816\n",
      "103 -0.693069 -0.529825 -0.929485  0.466760\n",
      "\n",
      "[104 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "kfold = RepeatedKFold(n_splits=5, n_repeats=100)\n",
    "\n",
    "# Type_error = 'mape'\n",
    "\n",
    "List_Error = ['mean_absolute_percentage_error', 'mean_squared_error']\n",
    "Type_error = 'mean_squared_error'\n",
    "\n",
    "# ['softmax', 'softplus', 'softsign']\n",
    "# Type_func = List_Function[0]\n",
    "# Type_optimizer='adam'   \n",
    "Type_optimizer='RMSprop'\n",
    "Number_epochs = 100\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "l_feat = X_train_scaled.shape[1]\n",
    "loss_func = 'mean_absolute_error'\n",
    "\n",
    "\n",
    "print(X_train_scaled)\n",
    "\n",
    "\n",
    "def network_model(l_r = 0.01, d_c = 0.001, reg = -1, activ = 'tanh',\\\n",
    "                  n_layer = 1, n_neur = 3, drop = 0):\n",
    "    network = models.Sequential()\n",
    "    for j in range(n_layer):\n",
    "        network.add(layers.Dense(units = n_neur, activation = activ, input_shape=(l_feat,),\\\n",
    "                                 kernel_regularizer=regularizers.L1(10**reg)))\n",
    "        network.add(Dropout(drop))\n",
    "    network.add(layers.Dense(units = 1, activation = 'linear'))\n",
    "    network.compile(loss = loss_func, optimizer = \\\n",
    "                    keras.optimizers.Adam(learning_rate = l_r, decay = d_c),\\\n",
    "                    metrics='mean_absolute_percentage_error') \n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c042e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-14 12:56:53.095581: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-14 12:56:53.095580: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-14 12:56:53.095580: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-14 12:56:53.147874: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "FFNN = KerasRegressor(build_fn = network_model, verbose=1)\n",
    "\n",
    "\n",
    "\n",
    "parameters = {\n",
    "                'l_r': [0.001,0.003,0.01, 0.03,  0.1],\n",
    "                'd_c': [0.001,0.003, 0.01,0.03],\n",
    "                'reg': [-1, -1.5,-2],\n",
    "                'n_layer': [1,2,3,4],\n",
    "                'n_neur': [2,3,4,5],\n",
    "                'activ': ['tanh','relu'],\n",
    "                'drop': [0, 0.2, 0.4, 0.6, 0.8],\n",
    "                'batch_size': [4,8,16,32],\n",
    "              # 'activ': ['linear', 'tanh', 'sigmoid', 'relu', 'exponential']\n",
    "                'epochs': [5,10,20,40,80]\n",
    "                }\n",
    "#                            scoring='neg_mean_squared_error',\n",
    "#                            scoring = \"neg_mean_absolute_percentage_error\",\n",
    "grid =  GridSearchCV(estimator = FFNN, param_grid = parameters,\\\n",
    "                     scoring = \"neg_mean_absolute_error\", cv = kfold,\\\n",
    "                     n_jobs = -1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "grid_result = grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "\n",
    "print(grid_result.best_params_)\n",
    "print(grid_result.best_score_)\n",
    "\n",
    "\n",
    "# print(grid_result.cv_results_)\n",
    "\n",
    "df_result = pd.DataFrame(grid_result.cv_results_)\n",
    "print(df_result)\n",
    "\n",
    "df_result.to_csv(\"GSCV/GSCV_1.csv\", index=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a66ebb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456a2926",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a46d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearDataset(Dataset):\n",
    "    def __init__(self, data_samples, data_targets):\n",
    "        self.samples = data_samples\n",
    "        self.targets = data_targets\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "\n",
    "def load_data(data_samples, data_targets, t_s = 0.33):\n",
    "    train_samples, valid_samples, train_targets, valid_targets =\\\n",
    "    train_test_split(data_samples, date_targets, test_size = t_s)\n",
    "        \n",
    "    train_data = LinearDataset(train_samples, train_targets)\n",
    "    valid_data = LinearDataset(valid_samples, valid_targets)\n",
    "\n",
    "    return train_data, test_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc0b8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_data(t_s = 0.33):\n",
    "#     X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = t_s)\n",
    "#     time_train = X_train.dropna()['date']\n",
    "#     X_train = X_train.drop(['date'], axis=1)\n",
    "#     time_valid = X_valid.dropna()['date']\n",
    "#     X_valid = X_valid.drop(['date'], axis=1)\n",
    "#     X_train['SO2'] = y_train\n",
    "#     X_valid['SO2'] = y_valid\n",
    "#     return X_train, X_valid\n",
    "\n",
    "\n",
    "# l_feat = X_train_scaled.shape[1]\n",
    "# print(l_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3577191e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(trial):\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "    drop_out = trial.suggest_float(\"drop_out\", 0, 1.0)\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    for i in range(n_layers):\n",
    "        num_hidden = trial.suggest_int(\"n_units_l{}\".format(i), 2, 5, log=False)\n",
    "        model.add(tf.keras.layers.Dense(num_hidden, activation=\"relu\"))\n",
    "        network.add(Dropout(drop_out))\n",
    "    \n",
    "    network.add(layers.Dense(units = 1, activation = 'linear'))\n",
    "#     network.compile(loss = 'mean_absolute_error', optimizer = \\\n",
    "#                     keras.optimizers.Adam(learning_rate = l_r, decay = d_c,\\\n",
    "#                                           beta_1 = b_1, beta_2 = b_2, epsilon = eps),\\\n",
    "#                     metrics='mean_absolute_percentage_error') \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffa3a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimizer(trial):\n",
    "    kwargs = {}\n",
    "    optimizer_options = [\"Adam\", \"SGD\"]\n",
    "    optimizer_selected = trial.suggest_categorical(\"optimizer\", optimizer_options)\n",
    "    if optimizer_selected == \"Adam\":\n",
    "        kwargs[\"learning_rate\"] = trial.suggest_float(\"adam_learning_rate\", 1e-5, 1e-1, log=True)\n",
    "        kwargs[\"decay\"] = trial.suggest_float(\"adam_decay\", 1e-4, 1e-1, log = True)\n",
    "    elif optimizer_selected == \"SGD\":\n",
    "        kwargs[\"learning_rate\"] = trial.suggest_float(\"SGD_learning_rate\", 1e-5, 1e-1, log=True)\n",
    "        kwargs[\"momentum\"] = trial.suggest_float('SGD_momentum',1e-3,1e-1,log = True)\n",
    "        kwargs[\"decay\"] = trial.suggest_float(\"adam_decay\", 1e-4, 1e-1, log = True)\n",
    "    optimizer = getattr(tf.optimizers, optimizer_selected)(**kwargs)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5485be08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(model, optimizer, dataset, mode=\"eval\"):\n",
    "    \n",
    "    mae = tf.keras.metrics.MeanAbsoluteError('mean_absolute_error', dtype=tf.float32)\n",
    "#     accuracy = tf.metrics.Accuracy(\"accuracy\", dtype=tf.float32)\n",
    "\n",
    "    for batch, (sampleas, targets) in enumerate(dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(sampleas, training=(mode == \"train\"))\n",
    "            loss_value = ...\n",
    "            ...\n",
    "\n",
    "    if mode == \"eval\":\n",
    "        return mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e2ed47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Get train/valid data.\n",
    "    train_ds, valid_ds = load_data(X,y)\n",
    "\n",
    "    # Build model and optimizer.\n",
    "    model = create_model(trial)\n",
    "    optimizer = create_optimizer(trial)\n",
    "\n",
    "    # Training and validating cycle.\n",
    "    for _ in range(epochs):\n",
    "        learn(model, optimizer, train_ds, \"train\")\n",
    "\n",
    "    accuracy = learn(model, optimizer, valid_ds, \"eval\")\n",
    "\n",
    "    # Return last validation accuracy.\n",
    "    return accuracy.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52287fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search():\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=100)\n",
    "\n",
    "    print(\"Number of finished trials: \", len(study.trials))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: \", trial.value)\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f881fe0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54c4e5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4fded1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d85e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, random_split, DataLoader \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "class LinearDataset(Dataset):\n",
    "    def __init__(self, data_samples, data_targets, device='cuda:0'):\n",
    "        self.samples = data_samples\n",
    "        self.targets = data_targets\n",
    "        self.device=device\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        target = self.targets[idx]\n",
    "        \n",
    "        return torch.tensor(sample, device=self.device), torch.tensor(target, device=self.device)\n",
    "\n",
    "\n",
    "def load_data(data_samples, data_targets):\n",
    "    \n",
    "    data = LinearDataset(data_samples, data_targets)\n",
    "\n",
    "    # the constants to define\n",
    "    test_rate = .2\n",
    "\n",
    "    # the constants to calculate\n",
    "    test_num = round(len(data)*test_rate)\n",
    "    train_num = len(data) - test_num\n",
    "\n",
    "    train_data, test_data = random_split(data, [train_num, test_num], generator=torch.Generator().manual_seed(42))   \n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023a90de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61fae0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1712cc7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4ef781",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_custom(trial):\n",
    "    \n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 3)\n",
    "    layers = []\n",
    "\n",
    "    in_features = 4\n",
    "    \n",
    "    for i in range(n_layers):\n",
    "        \n",
    "        out_features = trial.suggest_int(\"n_units_l{}\".format(i), 1, 5)\n",
    "        \n",
    "        layers.append(nn.Linear(in_features, out_features))\n",
    "        layers.append(nn.LeakyReLU())\n",
    "\n",
    "        in_features = out_features\n",
    "        \n",
    "    layers.append(nn.Linear(in_features, 1))\n",
    "    layers.append(nn.ReLU())\n",
    "    \n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545e9a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(param, model, trial):\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    criterion = nn.L1Loss()\n",
    "    optimizer = getattr(optim, param['optimizer'])(model.parameters(), lr= param['learning_rate'])\n",
    "\n",
    "    if use_cuda:\n",
    "            model = model.cuda()\n",
    "            criterion = criterion.cuda()\n",
    "\n",
    "    for epoch_num in range(EPOCHS):\n",
    "\n",
    "            total_acc_train = 0\n",
    "            total_loss_train = 0\n",
    "\n",
    "            for train_input, train_label in train_dataloader:\n",
    "\n",
    "                train_label = train_label.to(device)\n",
    "                train_input = train_input.to(device)\n",
    "\n",
    "                output = model(train_input.float())\n",
    "                \n",
    "                batch_loss = criterion(output, train_label.long())\n",
    "                total_loss_train += batch_loss.item()\n",
    "                \n",
    "                acc = (output.argmax(dim=1) == train_label).sum().item()\n",
    "                total_acc_train += acc\n",
    "\n",
    "                model.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            total_acc_val = 0\n",
    "            total_loss_val = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                for val_input, val_label in val_dataloader:\n",
    "\n",
    "                    val_label = val_label.to(device)\n",
    "                    val_input = val_input.to(device)\n",
    "\n",
    "                    output = model(val_input.float())\n",
    "\n",
    "                    batch_loss = criterion(output, val_label.long())\n",
    "                    total_loss_val += batch_loss.item()\n",
    "                    \n",
    "                    acc = (output.argmax(dim=1) == val_label).sum().item()\n",
    "                    total_acc_val += acc\n",
    "            \n",
    "            accuracy = total_acc_val/len(val_data)\n",
    "            \n",
    "            # Add prune mechanism\n",
    "            trial.report(accuracy, epoch_num)\n",
    "\n",
    "            if trial.should_prune():\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4401d06d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329bafe0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf09851",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2496d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f3d6a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b746c140",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d98aabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# Build a model by implementing define-by-run design from Optuna\n",
    "def build_model_custom(trial):\n",
    "    \n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 3)\n",
    "    layers = []\n",
    "\n",
    "    in_features = 4\n",
    "    \n",
    "    for i in range(n_layers):\n",
    "        \n",
    "        out_features = trial.suggest_int(\"n_units_l{}\".format(i), 1, 5)\n",
    "        \n",
    "        layers.append(nn.Linear(in_features, out_features))\n",
    "        layers.append(nn.LeakyReLU())\n",
    "\n",
    "        in_features = out_features\n",
    "        \n",
    "    layers.append(nn.Linear(in_features, 1))\n",
    "    layers.append(nn.ReLU())\n",
    "    \n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "# Train and evaluate the accuracy of neural network with the addition of pruning mechanism\n",
    "def train_and_evaluate(param, model, trial):\n",
    "    \n",
    "#     df = pd.read_csv('heart.csv')\n",
    "#     df = pd.get_dummies(df)\n",
    "    \n",
    "#     train_data, val_data = train_test_split(df, test_size = 0.2, random_state = 42)\n",
    "#     train, val = Dataset(train_data), Dataset(val_data)\n",
    "\n",
    "#     train_dataloader = torch.utils.data.DataLoader(train, batch_size=2, shuffle=True)\n",
    "#     val_dataloader = torch.utils.data.DataLoader(val, batch_size=2)\n",
    "\n",
    "    \n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=2, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=2)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "    criterion = nn.L1Loss()\n",
    "    optimizer = getattr(optim, param['optimizer'])(model.parameters(), lr= param['learning_rate'])\n",
    "\n",
    "    if use_cuda:\n",
    "            model = model.cuda()\n",
    "            criterion = criterion.cuda()\n",
    "\n",
    "    for epoch_num in range(EPOCHS):\n",
    "\n",
    "            total_acc_train = 0\n",
    "            total_loss_train = 0\n",
    "\n",
    "            for train_input, train_label in train_dataloader:\n",
    "\n",
    "                train_label = train_label.to(device)\n",
    "                train_input = train_input.to(device)\n",
    "\n",
    "                output = model(train_input.float())\n",
    "                \n",
    "                batch_loss = criterion(output, train_label.long())\n",
    "                total_loss_train += batch_loss.item()\n",
    "                \n",
    "                acc = (output.argmax(dim=1) == train_label).sum().item()\n",
    "                total_acc_train += acc\n",
    "\n",
    "                model.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            total_acc_val = 0\n",
    "            total_loss_val = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                for val_input, val_label in val_dataloader:\n",
    "\n",
    "                    val_label = val_label.to(device)\n",
    "                    val_input = val_input.to(device)\n",
    "\n",
    "                    output = model(val_input.float())\n",
    "\n",
    "                    batch_loss = criterion(output, val_label.long())\n",
    "                    total_loss_val += batch_loss.item()\n",
    "                    \n",
    "                    acc = (output.argmax(dim=1) == val_label).sum().item()\n",
    "                    total_acc_val += acc\n",
    "            \n",
    "            accuracy = total_acc_val/len(val_data)\n",
    "            \n",
    "            # Add prune mechanism\n",
    "            trial.report(accuracy, epoch_num)\n",
    "\n",
    "            if trial.should_prune():\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return accuracy\n",
    "  \n",
    "# Define a set of hyperparameter values, build the model, train the model, and evaluate the accuracy\n",
    "def objective(trial):\n",
    "\n",
    "     params = {\n",
    "              'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-1),\n",
    "              'optimizer': trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"]),\n",
    "              }\n",
    "    \n",
    "     model = build_model_custom(trial)\n",
    "\n",
    "     accuracy = train_and_evaluate(params, model, trial)\n",
    "\n",
    "     return accuracy\n",
    "  \n",
    "  \n",
    "EPOCHS = 30\n",
    "    \n",
    "study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner())\n",
    "study.optimize(objective, n_trials=30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce76ff6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8ecfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "kfold = RepeatedKFold(n_splits=5, n_repeats=100)\n",
    "from ann_visualizer.visualize import ann_viz\n",
    "\n",
    "# Type_error = 'mape'\n",
    "\n",
    "List_Error = ['mean_absolute_percentage_error', 'mean_squared_error']\n",
    "Type_error = 'mean_squared_error'\n",
    "\n",
    "# ['softmax', 'softplus', 'softsign']\n",
    "# Type_func = List_Function[0]\n",
    "# Type_optimizer='adam'   \n",
    "Type_optimizer='RMSprop'\n",
    "Number_epochs = 100\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "l_feat = X_train_scaled_poly.shape[1]\n",
    "activ_func = 'tanh'\n",
    "loss_func = 'mean_absolute_error'\n",
    "\n",
    "\n",
    "print(X_train_scaled_poly)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "b_1 = 0.9\n",
    "b_2 = 0.999\n",
    "eps = 1e-07\n",
    "lr = 0.005\n",
    "d = 0.005\n",
    "\n",
    "\n",
    "def network_model(l_r = 0.01, d_c = 0.001, b_1 = 0.9, b_2 = 0.999, eps = 1e-01, reg = -1, activ = 'tanh',n_layer = 1, n_neur = 10, drop = 0):\n",
    "    network = models.Sequential()\n",
    "    for j in range(n_layer):\n",
    "        network.add(layers.Dense(units = n_neur, activation = activ, input_shape=(l_feat,), kernel_regularizer=regularizers.L1(10**reg)))\n",
    "        network.add(Dropout(drop))\n",
    "    \n",
    "    network.add(layers.Dense(units = 1, activation = 'linear'))\n",
    "    network.compile(loss = 'mean_absolute_error', optimizer= keras.optimizers.Adam(learning_rate = l_r, decay = d_c, beta_1 = b_1, beta_2 = b_2, epsilon = eps), metrics='mean_absolute_percentage_error') \n",
    "    return network\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "FFNN = KerasRegressor(build_fn = network_model, verbose=0, batch_size = 50, epochs = 1000)\n",
    "\n",
    "\n",
    "\n",
    "parameters = {\n",
    "              # 'l_r': [0.01, 0.05,  0.1],\n",
    "              # 'd_c': [0.001,0.005, 0.01],\n",
    "              # 'b_1': [0.8, 0.9, 0.99],\n",
    "              # 'b_2': [0.9, 0.99, 0.999],\n",
    "\n",
    "              # 'reg': [-1, -1.5],\n",
    "              # 'eps': [1e-07, 1e-01],\n",
    "              'n_layer': [1,2,3,4],\n",
    "              'n_neur': [5,10,15],\n",
    "              'activ': ['tanh','relu'],\n",
    "              'drop': [0, 0.2, 0.4, 0.6, 0.8],\n",
    "            \n",
    "              # 'activ': ['linear', 'tanh', 'sigmoid', 'relu', 'exponential']\n",
    "              # 'epochs': [30, 100, 300, 1000],\n",
    "                }\n",
    "\n",
    "grid =  GridSearchCV(estimator = FFNN,\n",
    "                           param_grid = parameters,\n",
    "                           # scoring='neg_mean_squared_error',\n",
    "                           # scoring = \"neg_mean_absolute_error\",\n",
    "                           scoring = \"neg_mean_absolute_percentage_error\",\n",
    "                           cv = 10, \n",
    "                           n_jobs = -1)\n",
    "\n",
    "\n",
    "grid_result = grid.fit(X_train_scaled_poly, y_train)\n",
    "\n",
    "\n",
    "print(grid_result.best_params_)\n",
    "print(grid_result.best_score_)\n",
    "\n",
    "\n",
    "# print(grid_result.cv_results_)\n",
    "\n",
    "df_result = pd.DataFrame(grid_result.cv_results_)\n",
    "print(df_result)\n",
    "\n",
    "df_result.to_csv(\"GSCV/GSCV_1000_V7_ep.csv\", index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84a347d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9427114",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924d5015",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
