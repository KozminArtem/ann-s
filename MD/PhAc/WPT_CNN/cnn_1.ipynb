{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2b026b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module\n",
    "from torch.nn import Conv2d\n",
    "from torch.nn import Linear\n",
    "from torch.nn import MaxPool2d\n",
    "from torch.nn import ReLU\n",
    "# from torch.nn import LogSoftmax\n",
    "from torch import flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "39e63fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(Module):\n",
    "    def __init__(self):\n",
    "        # call the parent constructor\n",
    "        super(LeNet, self).__init__()\n",
    "\n",
    "        # initialize first set of CONV => RELU => POOL layers\n",
    "        self.conv1 = Conv2d(in_channels = 2, out_channels = 4,\\\n",
    "                            kernel_size = (2, 6), stride = (2,2))#2 - height, 6 - width \n",
    "        self.relu1 = ReLU()\n",
    "        self.maxpool1 = MaxPool2d(kernel_size=(3, 3), stride=(3, 3)) #stride = step\n",
    "        \n",
    "        \n",
    "        # initialize second set of CONV => RELU => POOL layers\n",
    "        self.conv2 = Conv2d(in_channels=4, out_channels=8,\\\n",
    "                            kernel_size=(4, 4))\n",
    "        self.relu2 = ReLU()\n",
    "        self.maxpool2 = MaxPool2d(kernel_size=(3, 3), stride=(3, 3))\n",
    "        \n",
    "        \n",
    "        # initialize third set of CONV => RELU => POOL layers\n",
    "        self.conv3 = Conv2d(in_channels=8, out_channels=16,\\\n",
    "                            kernel_size=(3, 3))\n",
    "        self.relu3 = ReLU()\n",
    "        self.maxpool3 = MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # initialize first (and only) set of FC => RELU layers\n",
    "        self.fc1 = Linear(in_features=32, out_features=8)\n",
    "        self.relu4 = ReLU()\n",
    "        # initialize our softmax classifier\n",
    "        self.fc2 = Linear(in_features=8, out_features=1)\n",
    "#         self.logSoftmax = LogSoftmax(dim=1)\n",
    "    def forward(self, x):\n",
    "        # pass the input through our first set of CONV => RELU =>\n",
    "        # POOL layers\n",
    "#         print(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.maxpool1(x)\n",
    "\n",
    "        # pass the output from the previous layer through the second\n",
    "        # set of CONV => RELU => POOL layers\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.maxpool2(x)\n",
    "\n",
    "        # pass the output from the previous layer through the second\n",
    "        # set of CONV => RELU => POOL layers\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.maxpool3(x)\n",
    "\n",
    "\n",
    "        # flatten the output from the previous layer and pass it\n",
    "        # through our only set of FC => RELU layers\n",
    "\n",
    "        x = flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu4(x)\n",
    "\n",
    "        # pass the output to our softmax classifier to get our output\n",
    "        # predictions\n",
    "        output = self.fc2(x)\n",
    "    #     output = self.logSoftmax(x)\n",
    "        # return the output predictions\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "1059a31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(Module):\n",
    "    def __init__(self):\n",
    "        # call the parent constructor\n",
    "        super(LeNet, self).__init__()\n",
    "\n",
    "        # initialize first set of CONV => RELU => POOL layers\n",
    "        self.conv1 = Conv2d(in_channels = 2, out_channels = 4,\\\n",
    "                            kernel_size = (4, 10), stride = (4,5))#2 - height, 6 - width \n",
    "        self.relu1 = ReLU()\n",
    "        self.maxpool1 = MaxPool2d(kernel_size=(4, 3), stride=(4, 3)) #stride = step\n",
    "        \n",
    "        \n",
    "        # initialize first (and only) set of FC => RELU layers\n",
    "        self.fc1 = Linear(in_features=48, out_features=8)\n",
    "        self.relu2 = ReLU()\n",
    "        self.fc2 = Linear(in_features=8, out_features=3)\n",
    "        self.relu3 = ReLU()\n",
    "        # initialize our softmax classifier\n",
    "        self.fc3 = Linear(in_features=3, out_features=1)\n",
    "#         self.logSoftmax = LogSoftmax(dim=1)\n",
    "    def forward(self, x):\n",
    "        # pass the input through our first set of CONV => RELU =>\n",
    "        # POOL layers\n",
    "#         print(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        \n",
    "        x = flatten(x, 1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.relu2(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu3(x)\n",
    "\n",
    "        output = self.fc3(x)\n",
    "    #     output = self.logSoftmax(x)\n",
    "        # return the output predictions\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "26d4cd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the matplotlib backend so figures can be saved in the background\n",
    "import matplotlib\n",
    "# matplotlib.use(\"Agg\")\n",
    "\n",
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "import glob\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "1e417f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# set the device we will be using to train the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "326d914f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.data = []\n",
    "\n",
    "        target_954 = np.load('data_segment/Y_natur_954.npz')\n",
    "        for j in range(3000):\n",
    "            self.data.append([\"data_segment/natur_X/954_\" + str(j) + '.npz',target_954['arr_0'][j]])\n",
    "#         print(self.data)\n",
    "\n",
    "        target_9_7 = np.load('data_segment/Y_natur_9_7.npz')\n",
    "#         print(target_9_7['arr_0'])\n",
    "        for j in range(3000):\n",
    "            self.data.append([\"data_segment/natur_X/9_7_\" + str(j) + '.npz',target_9_7['arr_0'][j]])\n",
    "\n",
    "        \n",
    "        target_Air = np.load('data_segment/Y_natur_Air.npz')\n",
    "#         print(target_Air['arr_0'])\n",
    "        for j in range(3000):\n",
    "            self.data.append([\"data_segment/natur_X/Air_\" + str(j) + '.npz',target_Air['arr_0'][j]])\n",
    "        \n",
    "        temp = np.load('data_segment/natur_X/954_0.npz')\n",
    "        self.img_dim = temp['arr_0'].shape  \n",
    "        print(self.img_dim)\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)    \n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, target = self.data[idx]\n",
    "        \n",
    "        img = np.load(img_path)\n",
    "        img_tensor = torch.from_numpy(img['arr_0'])\n",
    "        \n",
    "        target_tensor = torch.tensor(target)\n",
    "#         print(target_tensor, target)\n",
    "        \n",
    "#         img_tensor = img_tensor.permute(2, 0, 1)\n",
    "#         class_id = torch.tensor([class_id])\n",
    "        \n",
    "        return img_tensor, target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "53786f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 64, 50)\n",
      "9000\n"
     ]
    }
   ],
   "source": [
    "data = CustomDataset()\n",
    "\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "5c8ff5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 64, 50])\n",
      "torch.DoubleTensor\n",
      "torch.DoubleTensor\n",
      "tensor(0.0319, dtype=torch.float64) 0.03186977247219236\n"
     ]
    }
   ],
   "source": [
    "print(data[1][0].size())\n",
    "print(data[1][0].type())\n",
    "print(data[1][1].type())\n",
    "print(data[1][1], data[1][1].item())\n",
    "# print(data[1][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "3308167f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_split = 0.7\n",
    "Val_split = 0.15\n",
    "Test_split = 0.15\n",
    "\n",
    "(trainData, valData, testData) = random_split(data, [Train_split, Val_split, Test_split],\\\n",
    "                                              generator=torch.Generator().manual_seed(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "44596b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6300\n",
      "1350\n",
      "1350\n"
     ]
    }
   ],
   "source": [
    "print(len(trainData))\n",
    "print(len(valData))\n",
    "print(len(testData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "7a2d5bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196\n",
      "42\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "trainDataLoader = DataLoader(trainData, shuffle=True, batch_size=BATCH_SIZE)\n",
    "valDataLoader = DataLoader(valData, batch_size=BATCH_SIZE)\n",
    "testDataLoader = DataLoader(testData, batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "\n",
    "# print(testDataLoader.dataset[0])\n",
    "\n",
    "# calculate steps per epoch for training and validation set\n",
    "trainSteps = len(trainDataLoader.dataset) // BATCH_SIZE\n",
    "print(trainSteps)\n",
    "\n",
    "valSteps = len(valDataLoader.dataset) // BATCH_SIZE\n",
    "print(valSteps)\n",
    "\n",
    "\n",
    "# print(trainDataLoader.dataset)\n",
    "# print(trainDataLoader.dataset[1])\n",
    "# print(trainDataLoader.dataset[1][0].type())\n",
    "# print(trainDataLoader.dataset[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "baeee3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] initializing the LeNet model...\n",
      "LeNet(\n",
      "  (conv1): Conv2d(2, 4, kernel_size=(4, 10), stride=(4, 5))\n",
      "  (relu1): ReLU()\n",
      "  (maxpool1): MaxPool2d(kernel_size=(4, 3), stride=(4, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=48, out_features=8, bias=True)\n",
      "  (relu2): ReLU()\n",
      "  (fc2): Linear(in_features=8, out_features=3, bias=True)\n",
      "  (relu3): ReLU()\n",
      "  (fc3): Linear(in_features=3, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define training hyperparameters\n",
    "INIT_LR = 0.001\n",
    "\n",
    "# initialize the LeNet model\n",
    "print(\"[INFO] initializing the LeNet model...\")\n",
    "\n",
    "model = LeNet().to(device)\n",
    "model.type(torch.cuda.DoubleTensor)\n",
    "\n",
    "\n",
    "print(model)\n",
    "# initialize our optimizer and loss function\n",
    "opt = Adam(model.parameters(), lr=INIT_LR)\n",
    "\n",
    "lossMSE = nn.MSELoss()\n",
    "# initialize a dictionary to store training history\n",
    "H = {\"train_loss\": [],\"val_loss\": []}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "d9cc0660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training the network...\n",
      "[INFO] EPOCH: 1/1000\n",
      "Train loss: 634244408750.174805, Val loss: 1802674406.604646\n",
      "[INFO] EPOCH: 2/1000\n",
      "Train loss: 971116383.209172, Val loss: 521576400.679469\n",
      "[INFO] EPOCH: 3/1000\n",
      "Train loss: 382461134.446445, Val loss: 232643548.651361\n",
      "[INFO] EPOCH: 4/1000\n",
      "Train loss: 204018094.042604, Val loss: 131053008.638063\n",
      "[INFO] EPOCH: 5/1000\n",
      "Train loss: 121418420.192680, Val loss: 82056544.798020\n",
      "[INFO] EPOCH: 6/1000\n",
      "Train loss: 70874601.375135, Val loss: 55067712.851525\n",
      "[INFO] EPOCH: 7/1000\n",
      "Train loss: 48097456.166298, Val loss: 39013192.240889\n",
      "[INFO] EPOCH: 8/1000\n",
      "Train loss: 33993820.734664, Val loss: 28397156.034711\n",
      "[INFO] EPOCH: 9/1000\n",
      "Train loss: 24949802.912529, Val loss: 21312721.246306\n",
      "[INFO] EPOCH: 10/1000\n",
      "Train loss: 18896186.468332, Val loss: 16268868.863772\n",
      "[INFO] EPOCH: 11/1000\n",
      "Train loss: 14664987.342819, Val loss: 12666710.197056\n",
      "[INFO] EPOCH: 12/1000\n",
      "Train loss: 11572934.457703, Val loss: 10044826.525310\n",
      "[INFO] EPOCH: 13/1000\n",
      "Train loss: 9296638.014802, Val loss: 8021439.355698\n",
      "[INFO] EPOCH: 14/1000\n",
      "Train loss: 7473747.081086, Val loss: 6391914.511473\n",
      "[INFO] EPOCH: 15/1000\n",
      "Train loss: 6060478.443605, Val loss: 5168876.267107\n",
      "[INFO] EPOCH: 16/1000\n",
      "Train loss: 4965358.291761, Val loss: 4183844.793313\n",
      "[INFO] EPOCH: 17/1000\n",
      "Train loss: 4096554.157845, Val loss: 3465156.963576\n",
      "[INFO] EPOCH: 18/1000\n",
      "Train loss: 3386994.192537, Val loss: 2823595.850027\n",
      "[INFO] EPOCH: 19/1000\n",
      "Train loss: 2815267.120127, Val loss: 2336263.053052\n",
      "[INFO] EPOCH: 20/1000\n",
      "Train loss: 2347220.721293, Val loss: 1917055.644755\n",
      "[INFO] EPOCH: 21/1000\n",
      "Train loss: 1966894.702369, Val loss: 1602474.757909\n",
      "[INFO] EPOCH: 22/1000\n",
      "Train loss: 1795431.418650, Val loss: 1354163.083292\n",
      "[INFO] EPOCH: 23/1000\n",
      "Train loss: 1432810.506258, Val loss: 1124170.322052\n",
      "[INFO] EPOCH: 24/1000\n",
      "Train loss: 1210074.776674, Val loss: 934387.394850\n",
      "[INFO] EPOCH: 25/1000\n",
      "Train loss: 1020952.242117, Val loss: 786430.988230\n",
      "[INFO] EPOCH: 26/1000\n",
      "Train loss: 863134.710464, Val loss: 660104.131376\n",
      "[INFO] EPOCH: 27/1000\n",
      "Train loss: 733380.880621, Val loss: 555610.674172\n",
      "[INFO] EPOCH: 28/1000\n",
      "Train loss: 617205.977607, Val loss: 461314.559557\n",
      "[INFO] EPOCH: 29/1000\n",
      "Train loss: 521863.050888, Val loss: 387497.823157\n",
      "[INFO] EPOCH: 30/1000\n",
      "Train loss: 440599.883318, Val loss: 323244.663012\n",
      "[INFO] EPOCH: 31/1000\n",
      "Train loss: 371829.748683, Val loss: 271850.091083\n",
      "[INFO] EPOCH: 32/1000\n",
      "Train loss: 313610.757482, Val loss: 227927.033466\n",
      "[INFO] EPOCH: 33/1000\n",
      "Train loss: 263650.847039, Val loss: 191085.326240\n",
      "[INFO] EPOCH: 34/1000\n",
      "Train loss: 220916.951665, Val loss: 158089.868829\n",
      "[INFO] EPOCH: 35/1000\n",
      "Train loss: 185366.752660, Val loss: 131568.946201\n",
      "[INFO] EPOCH: 36/1000\n",
      "Train loss: 156095.098753, Val loss: 109166.939262\n",
      "[INFO] EPOCH: 37/1000\n",
      "Train loss: 129883.597433, Val loss: 91829.105209\n",
      "[INFO] EPOCH: 38/1000\n",
      "Train loss: 108358.520584, Val loss: 73865.701662\n",
      "[INFO] EPOCH: 39/1000\n",
      "Train loss: 90675.345899, Val loss: 60234.188426\n",
      "[INFO] EPOCH: 40/1000\n",
      "Train loss: 75452.972588, Val loss: 49178.168878\n",
      "[INFO] EPOCH: 41/1000\n",
      "Train loss: 62617.609209, Val loss: 39291.627946\n",
      "[INFO] EPOCH: 42/1000\n",
      "Train loss: 51392.727233, Val loss: 31404.196150\n",
      "[INFO] EPOCH: 43/1000\n",
      "Train loss: 41952.500516, Val loss: 24832.495487\n",
      "[INFO] EPOCH: 44/1000\n",
      "Train loss: 34365.539585, Val loss: 19214.296614\n",
      "[INFO] EPOCH: 45/1000\n",
      "Train loss: 27697.664985, Val loss: 15401.871295\n",
      "[INFO] EPOCH: 46/1000\n",
      "Train loss: 22475.118637, Val loss: 12264.856649\n",
      "[INFO] EPOCH: 47/1000\n",
      "Train loss: 18239.675399, Val loss: 9775.181067\n",
      "[INFO] EPOCH: 48/1000\n",
      "Train loss: 14689.170778, Val loss: 7975.354480\n",
      "[INFO] EPOCH: 49/1000\n",
      "Train loss: 11902.083019, Val loss: 6064.853450\n",
      "[INFO] EPOCH: 50/1000\n",
      "Train loss: 9634.371152, Val loss: 4729.160066\n",
      "[INFO] EPOCH: 51/1000\n",
      "Train loss: 7805.801428, Val loss: 3645.050180\n",
      "[INFO] EPOCH: 52/1000\n",
      "Train loss: 6437.576324, Val loss: 2779.396749\n",
      "[INFO] EPOCH: 53/1000\n",
      "Train loss: 5277.007583, Val loss: 2170.944080\n",
      "[INFO] EPOCH: 54/1000\n",
      "Train loss: 4354.099098, Val loss: 1619.613619\n",
      "[INFO] EPOCH: 55/1000\n",
      "Train loss: 3614.898351, Val loss: 1148.512334\n",
      "[INFO] EPOCH: 56/1000\n",
      "Train loss: 3018.423714, Val loss: 813.606762\n",
      "[INFO] EPOCH: 57/1000\n",
      "Train loss: 2523.368946, Val loss: 556.160983\n",
      "[INFO] EPOCH: 58/1000\n",
      "Train loss: 2081.850204, Val loss: 355.113047\n",
      "[INFO] EPOCH: 59/1000\n",
      "Train loss: 1691.224048, Val loss: 168.533973\n",
      "[INFO] EPOCH: 60/1000\n",
      "Train loss: 1363.224585, Val loss: 52.734499\n",
      "[INFO] EPOCH: 61/1000\n",
      "Train loss: 1096.609978, Val loss: 29.051335\n",
      "[INFO] EPOCH: 62/1000\n",
      "Train loss: 864.944828, Val loss: 14.212272\n",
      "[INFO] EPOCH: 63/1000\n",
      "Train loss: 671.169533, Val loss: 7.254318\n",
      "[INFO] EPOCH: 64/1000\n",
      "Train loss: 534.377513, Val loss: 4.517519\n",
      "[INFO] EPOCH: 65/1000\n",
      "Train loss: 418.845809, Val loss: 0.448487\n",
      "[INFO] EPOCH: 66/1000\n",
      "Train loss: 322.616785, Val loss: 0.056783\n",
      "[INFO] EPOCH: 67/1000\n",
      "Train loss: 254.995958, Val loss: 0.056803\n",
      "[INFO] EPOCH: 68/1000\n",
      "Train loss: 204.374290, Val loss: 0.056805\n",
      "[INFO] EPOCH: 69/1000\n",
      "Train loss: 165.271126, Val loss: 0.056785\n",
      "[INFO] EPOCH: 70/1000\n",
      "Train loss: 132.769300, Val loss: 0.056784\n",
      "[INFO] EPOCH: 71/1000\n",
      "Train loss: 109.347699, Val loss: 0.056758\n",
      "[INFO] EPOCH: 72/1000\n",
      "Train loss: 91.178437, Val loss: 0.056718\n",
      "[INFO] EPOCH: 73/1000\n",
      "Train loss: 76.032173, Val loss: 0.056666\n",
      "[INFO] EPOCH: 74/1000\n",
      "Train loss: 63.270638, Val loss: 0.056603\n",
      "[INFO] EPOCH: 75/1000\n",
      "Train loss: 51.884302, Val loss: 0.056529\n",
      "[INFO] EPOCH: 76/1000\n",
      "Train loss: 47.443842, Val loss: 0.056404\n",
      "[INFO] EPOCH: 77/1000\n",
      "Train loss: 30.439617, Val loss: 0.056344\n",
      "[INFO] EPOCH: 78/1000\n",
      "Train loss: 22.647548, Val loss: 0.056236\n",
      "[INFO] EPOCH: 79/1000\n",
      "Train loss: 16.673583, Val loss: 0.056100\n",
      "[INFO] EPOCH: 80/1000\n",
      "Train loss: 11.775088, Val loss: 0.055944\n",
      "[INFO] EPOCH: 81/1000\n",
      "Train loss: 8.027663, Val loss: 0.055764\n",
      "[INFO] EPOCH: 82/1000\n",
      "Train loss: 5.306405, Val loss: 0.055561\n",
      "[INFO] EPOCH: 83/1000\n",
      "Train loss: 3.290651, Val loss: 0.055331\n",
      "[INFO] EPOCH: 84/1000\n",
      "Train loss: 1.978612, Val loss: 0.055066\n",
      "[INFO] EPOCH: 85/1000\n",
      "Train loss: 1.066759, Val loss: 0.054783\n",
      "[INFO] EPOCH: 86/1000\n",
      "Train loss: 0.557330, Val loss: 0.054459\n",
      "[INFO] EPOCH: 87/1000\n",
      "Train loss: 0.283589, Val loss: 0.054099\n",
      "[INFO] EPOCH: 88/1000\n",
      "Train loss: 0.150453, Val loss: 0.053701\n",
      "[INFO] EPOCH: 89/1000\n",
      "Train loss: 0.089983, Val loss: 0.053262\n",
      "[INFO] EPOCH: 90/1000\n",
      "Train loss: 0.064444, Val loss: 0.052780\n",
      "[INFO] EPOCH: 91/1000\n",
      "Train loss: 0.054958, Val loss: 0.052251\n",
      "[INFO] EPOCH: 92/1000\n",
      "Train loss: 0.051692, Val loss: 0.051673\n",
      "[INFO] EPOCH: 93/1000\n",
      "Train loss: 0.050426, Val loss: 0.051041\n",
      "[INFO] EPOCH: 94/1000\n",
      "Train loss: 0.049641, Val loss: 0.050354\n",
      "[INFO] EPOCH: 95/1000\n",
      "Train loss: 0.048922, Val loss: 0.049606\n",
      "[INFO] EPOCH: 96/1000\n",
      "Train loss: 0.048157, Val loss: 0.048793\n",
      "[INFO] EPOCH: 97/1000\n",
      "Train loss: 0.047325, Val loss: 0.047910\n",
      "[INFO] EPOCH: 98/1000\n",
      "Train loss: 0.046426, Val loss: 0.046956\n",
      "[INFO] EPOCH: 99/1000\n",
      "Train loss: 0.045455, Val loss: 0.045925\n",
      "[INFO] EPOCH: 100/1000\n",
      "Train loss: 0.044403, Val loss: 0.044813\n",
      "[INFO] EPOCH: 101/1000\n",
      "Train loss: 0.043275, Val loss: 0.043618\n",
      "[INFO] EPOCH: 102/1000\n",
      "Train loss: 0.042061, Val loss: 0.042335\n",
      "[INFO] EPOCH: 103/1000\n",
      "Train loss: 0.040760, Val loss: 0.040964\n",
      "[INFO] EPOCH: 104/1000\n",
      "Train loss: 0.039373, Val loss: 0.039503\n",
      "[INFO] EPOCH: 105/1000\n",
      "Train loss: 0.037897, Val loss: 0.037951\n",
      "[INFO] EPOCH: 106/1000\n",
      "Train loss: 0.036331, Val loss: 0.036309\n",
      "[INFO] EPOCH: 107/1000\n",
      "Train loss: 0.034680, Val loss: 0.034580\n",
      "[INFO] EPOCH: 108/1000\n",
      "Train loss: 0.032946, Val loss: 0.032768\n",
      "[INFO] EPOCH: 109/1000\n",
      "Train loss: 0.031133, Val loss: 0.030880\n",
      "[INFO] EPOCH: 110/1000\n",
      "Train loss: 0.029247, Val loss: 0.028922\n",
      "[INFO] EPOCH: 111/1000\n",
      "Train loss: 0.027301, Val loss: 0.026907\n",
      "[INFO] EPOCH: 112/1000\n",
      "Train loss: 0.025307, Val loss: 0.024846\n",
      "[INFO] EPOCH: 113/1000\n",
      "Train loss: 0.023272, Val loss: 0.022756\n",
      "[INFO] EPOCH: 114/1000\n",
      "Train loss: 0.021218, Val loss: 0.020655\n",
      "[INFO] EPOCH: 115/1000\n",
      "Train loss: 0.019163, Val loss: 0.018564\n",
      "[INFO] EPOCH: 116/1000\n",
      "Train loss: 0.017128, Val loss: 0.016504\n",
      "[INFO] EPOCH: 117/1000\n",
      "Train loss: 0.015135, Val loss: 0.014497\n",
      "[INFO] EPOCH: 118/1000\n",
      "Train loss: 0.013207, Val loss: 0.012569\n",
      "[INFO] EPOCH: 119/1000\n",
      "Train loss: 0.011366, Val loss: 0.010743\n",
      "[INFO] EPOCH: 120/1000\n",
      "Train loss: 0.009638, Val loss: 0.009039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 121/1000\n",
      "Train loss: 0.008039, Val loss: 0.007477\n",
      "[INFO] EPOCH: 122/1000\n",
      "Train loss: 0.006587, Val loss: 0.006074\n",
      "[INFO] EPOCH: 123/1000\n",
      "Train loss: 0.005297, Val loss: 0.004839\n",
      "[INFO] EPOCH: 124/1000\n",
      "Train loss: 0.004174, Val loss: 0.003777\n",
      "[INFO] EPOCH: 125/1000\n",
      "Train loss: 0.003221, Val loss: 0.002887\n",
      "[INFO] EPOCH: 126/1000\n",
      "Train loss: 0.002434, Val loss: 0.002163\n",
      "[INFO] EPOCH: 127/1000\n",
      "Train loss: 0.001803, Val loss: 0.001591\n",
      "[INFO] EPOCH: 128/1000\n",
      "Train loss: 0.001313, Val loss: 0.001154\n",
      "[INFO] EPOCH: 129/1000\n",
      "Train loss: 0.000947, Val loss: 0.000834\n",
      "[INFO] EPOCH: 130/1000\n",
      "Train loss: 0.000685, Val loss: 0.000608\n",
      "[INFO] EPOCH: 131/1000\n",
      "Train loss: 0.000504, Val loss: 0.000457\n",
      "[INFO] EPOCH: 132/1000\n",
      "Train loss: 0.000386, Val loss: 0.000360\n",
      "[INFO] EPOCH: 133/1000\n",
      "Train loss: 0.000313, Val loss: 0.000302\n",
      "[INFO] EPOCH: 134/1000\n",
      "Train loss: 0.000271, Val loss: 0.000268\n",
      "[INFO] EPOCH: 135/1000\n",
      "Train loss: 0.000247, Val loss: 0.000251\n",
      "[INFO] EPOCH: 136/1000\n",
      "Train loss: 0.000236, Val loss: 0.000242\n",
      "[INFO] EPOCH: 137/1000\n",
      "Train loss: 0.000230, Val loss: 0.000237\n",
      "[INFO] EPOCH: 138/1000\n",
      "Train loss: 0.000228, Val loss: 0.000236\n",
      "[INFO] EPOCH: 139/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 140/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 141/1000\n",
      "Train loss: 0.000226, Val loss: 0.000234\n",
      "[INFO] EPOCH: 142/1000\n",
      "Train loss: 0.000226, Val loss: 0.000234\n",
      "[INFO] EPOCH: 143/1000\n",
      "Train loss: 0.000226, Val loss: 0.000234\n",
      "[INFO] EPOCH: 144/1000\n",
      "Train loss: 0.000226, Val loss: 0.000234\n",
      "[INFO] EPOCH: 145/1000\n",
      "Train loss: 0.000226, Val loss: 0.000234\n",
      "[INFO] EPOCH: 146/1000\n",
      "Train loss: 0.000226, Val loss: 0.000234\n",
      "[INFO] EPOCH: 147/1000\n",
      "Train loss: 0.000226, Val loss: 0.000234\n",
      "[INFO] EPOCH: 148/1000\n",
      "Train loss: 0.000226, Val loss: 0.000234\n",
      "[INFO] EPOCH: 149/1000\n",
      "Train loss: 0.000226, Val loss: 0.000234\n",
      "[INFO] EPOCH: 150/1000\n",
      "Train loss: 0.000226, Val loss: 0.000234\n",
      "[INFO] EPOCH: 151/1000\n",
      "Train loss: 0.000226, Val loss: 0.000234\n",
      "[INFO] EPOCH: 152/1000\n",
      "Train loss: 0.000226, Val loss: 0.000234\n",
      "[INFO] EPOCH: 153/1000\n",
      "Train loss: 0.000226, Val loss: 0.000234\n",
      "[INFO] EPOCH: 154/1000\n",
      "Train loss: 0.000226, Val loss: 0.000234\n",
      "[INFO] EPOCH: 155/1000\n",
      "Train loss: 0.000226, Val loss: 0.000234\n",
      "[INFO] EPOCH: 156/1000\n",
      "Train loss: 0.000226, Val loss: 0.000234\n",
      "[INFO] EPOCH: 157/1000\n",
      "Train loss: 0.000226, Val loss: 0.000234\n",
      "[INFO] EPOCH: 158/1000\n",
      "Train loss: 0.000226, Val loss: 0.000234\n",
      "[INFO] EPOCH: 159/1000\n",
      "Train loss: 0.000226, Val loss: 0.000234\n",
      "[INFO] EPOCH: 160/1000\n",
      "Train loss: 0.000226, Val loss: 0.000234\n",
      "[INFO] EPOCH: 161/1000\n",
      "Train loss: 0.000226, Val loss: 0.000234\n",
      "[INFO] EPOCH: 162/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 163/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 164/1000\n",
      "Train loss: 0.000226, Val loss: 0.000234\n",
      "[INFO] EPOCH: 165/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 166/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 167/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 168/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 169/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 170/1000\n",
      "Train loss: 0.000226, Val loss: 0.000234\n",
      "[INFO] EPOCH: 171/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 172/1000\n",
      "Train loss: 0.000226, Val loss: 0.000235\n",
      "[INFO] EPOCH: 173/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 174/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 175/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 176/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 177/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 178/1000\n",
      "Train loss: 0.000227, Val loss: 0.000236\n",
      "[INFO] EPOCH: 179/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 180/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 181/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 182/1000\n",
      "Train loss: 0.000226, Val loss: 0.000234\n",
      "[INFO] EPOCH: 183/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 184/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 185/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 186/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 187/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 188/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 189/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 190/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 191/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 192/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 193/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 194/1000\n",
      "Train loss: 0.000228, Val loss: 0.000236\n",
      "[INFO] EPOCH: 195/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 196/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 197/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 198/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 199/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 200/1000\n",
      "Train loss: 0.000228, Val loss: 0.000236\n",
      "[INFO] EPOCH: 201/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 202/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 203/1000\n",
      "Train loss: 0.000228, Val loss: 0.000236\n",
      "[INFO] EPOCH: 204/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 205/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 206/1000\n",
      "Train loss: 0.000229, Val loss: 0.000243\n",
      "[INFO] EPOCH: 207/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 208/1000\n",
      "Train loss: 0.000228, Val loss: 0.000240\n",
      "[INFO] EPOCH: 209/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 210/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 211/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 212/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 213/1000\n",
      "Train loss: 0.000227, Val loss: 0.000237\n",
      "[INFO] EPOCH: 214/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 215/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 216/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 217/1000\n",
      "Train loss: 0.000226, Val loss: 0.000239\n",
      "[INFO] EPOCH: 218/1000\n",
      "Train loss: 0.000229, Val loss: 0.000234\n",
      "[INFO] EPOCH: 219/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 220/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 221/1000\n",
      "Train loss: 0.000228, Val loss: 0.000236\n",
      "[INFO] EPOCH: 222/1000\n",
      "Train loss: 0.000229, Val loss: 0.000236\n",
      "[INFO] EPOCH: 223/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 224/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 225/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 226/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 227/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 228/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 229/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 230/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 231/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 232/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 233/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 234/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 235/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 236/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 237/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 238/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 239/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 240/1000\n",
      "Train loss: 0.000228, Val loss: 0.000238\n",
      "[INFO] EPOCH: 241/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 242/1000\n",
      "Train loss: 0.000228, Val loss: 0.000237\n",
      "[INFO] EPOCH: 243/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 244/1000\n",
      "Train loss: 0.000227, Val loss: 0.000241\n",
      "[INFO] EPOCH: 245/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 246/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 247/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 248/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 249/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 250/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 251/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 252/1000\n",
      "Train loss: 0.000227, Val loss: 0.000237\n",
      "[INFO] EPOCH: 253/1000\n",
      "Train loss: 0.000227, Val loss: 0.000240\n",
      "[INFO] EPOCH: 254/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 255/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 256/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 257/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 258/1000\n",
      "Train loss: 0.000228, Val loss: 0.000236\n",
      "[INFO] EPOCH: 259/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 260/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 261/1000\n",
      "Train loss: 0.000228, Val loss: 0.000239\n",
      "[INFO] EPOCH: 262/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 263/1000\n",
      "Train loss: 0.000227, Val loss: 0.000243\n",
      "[INFO] EPOCH: 264/1000\n",
      "Train loss: 0.000228, Val loss: 0.000236\n",
      "[INFO] EPOCH: 265/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 266/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 267/1000\n",
      "Train loss: 0.000228, Val loss: 0.000236\n",
      "[INFO] EPOCH: 268/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 269/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 270/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 271/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 272/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 273/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 274/1000\n",
      "Train loss: 0.000228, Val loss: 0.000236\n",
      "[INFO] EPOCH: 275/1000\n",
      "Train loss: 0.000227, Val loss: 0.000242\n",
      "[INFO] EPOCH: 276/1000\n",
      "Train loss: 0.000228, Val loss: 0.000236\n",
      "[INFO] EPOCH: 277/1000\n",
      "Train loss: 0.000227, Val loss: 0.000239\n",
      "[INFO] EPOCH: 278/1000\n",
      "Train loss: 0.000228, Val loss: 0.000236\n",
      "[INFO] EPOCH: 279/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 280/1000\n",
      "Train loss: 0.000228, Val loss: 0.000236\n",
      "[INFO] EPOCH: 281/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 282/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 283/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 284/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 285/1000\n",
      "Train loss: 0.000228, Val loss: 0.000238\n",
      "[INFO] EPOCH: 286/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 287/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 288/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 289/1000\n",
      "Train loss: 0.000227, Val loss: 0.000236\n",
      "[INFO] EPOCH: 290/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 291/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 292/1000\n",
      "Train loss: 0.000227, Val loss: 0.000237\n",
      "[INFO] EPOCH: 293/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 294/1000\n",
      "Train loss: 0.000229, Val loss: 0.000234\n",
      "[INFO] EPOCH: 295/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 296/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 297/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 298/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 299/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 300/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 301/1000\n",
      "Train loss: 0.000229, Val loss: 0.000234\n",
      "[INFO] EPOCH: 302/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 303/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 304/1000\n",
      "Train loss: 0.000228, Val loss: 0.000238\n",
      "[INFO] EPOCH: 305/1000\n",
      "Train loss: 0.000228, Val loss: 0.000237\n",
      "[INFO] EPOCH: 306/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 307/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 308/1000\n",
      "Train loss: 0.000227, Val loss: 0.000236\n",
      "[INFO] EPOCH: 309/1000\n",
      "Train loss: 0.000227, Val loss: 0.000236\n",
      "[INFO] EPOCH: 310/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 311/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 312/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 313/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 314/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 315/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 316/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 317/1000\n",
      "Train loss: 0.000228, Val loss: 0.000241\n",
      "[INFO] EPOCH: 318/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 319/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 320/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 321/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 322/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 323/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 324/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 325/1000\n",
      "Train loss: 0.000227, Val loss: 0.000236\n",
      "[INFO] EPOCH: 326/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 327/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 328/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 329/1000\n",
      "Train loss: 0.000228, Val loss: 0.000236\n",
      "[INFO] EPOCH: 330/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 331/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 332/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 333/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 334/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 335/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 336/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 337/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 338/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 339/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 340/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 341/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 342/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 343/1000\n",
      "Train loss: 0.000228, Val loss: 0.000237\n",
      "[INFO] EPOCH: 344/1000\n",
      "Train loss: 0.000228, Val loss: 0.000237\n",
      "[INFO] EPOCH: 345/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 346/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 347/1000\n",
      "Train loss: 0.000227, Val loss: 0.000237\n",
      "[INFO] EPOCH: 348/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 349/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 350/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 351/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 352/1000\n",
      "Train loss: 0.000228, Val loss: 0.000237\n",
      "[INFO] EPOCH: 353/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 354/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 355/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 356/1000\n",
      "Train loss: 0.000228, Val loss: 0.000237\n",
      "[INFO] EPOCH: 357/1000\n",
      "Train loss: 0.000228, Val loss: 0.000238\n",
      "[INFO] EPOCH: 358/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 359/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 360/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 361/1000\n",
      "Train loss: 0.000228, Val loss: 0.000237\n",
      "[INFO] EPOCH: 362/1000\n",
      "Train loss: 0.000228, Val loss: 0.000238\n",
      "[INFO] EPOCH: 363/1000\n",
      "Train loss: 0.000227, Val loss: 0.000236\n",
      "[INFO] EPOCH: 364/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 365/1000\n",
      "Train loss: 0.000228, Val loss: 0.000237\n",
      "[INFO] EPOCH: 366/1000\n",
      "Train loss: 0.000228, Val loss: 0.000237\n",
      "[INFO] EPOCH: 367/1000\n",
      "Train loss: 0.000229, Val loss: 0.000235\n",
      "[INFO] EPOCH: 368/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 369/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 370/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 371/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 372/1000\n",
      "Train loss: 0.000228, Val loss: 0.000236\n",
      "[INFO] EPOCH: 373/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 374/1000\n",
      "Train loss: 0.000228, Val loss: 0.000242\n",
      "[INFO] EPOCH: 375/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 376/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 377/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 378/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 379/1000\n",
      "Train loss: 0.000227, Val loss: 0.000240\n",
      "[INFO] EPOCH: 380/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 381/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 382/1000\n",
      "Train loss: 0.000228, Val loss: 0.000237\n",
      "[INFO] EPOCH: 383/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 384/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 385/1000\n",
      "Train loss: 0.000227, Val loss: 0.000237\n",
      "[INFO] EPOCH: 386/1000\n",
      "Train loss: 0.000228, Val loss: 0.000236\n",
      "[INFO] EPOCH: 387/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 388/1000\n",
      "Train loss: 0.000227, Val loss: 0.000236\n",
      "[INFO] EPOCH: 389/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 390/1000\n",
      "Train loss: 0.000228, Val loss: 0.000237\n",
      "[INFO] EPOCH: 391/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 392/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 393/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 394/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 395/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 396/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 397/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 398/1000\n",
      "Train loss: 0.000228, Val loss: 0.000241\n",
      "[INFO] EPOCH: 399/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 400/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 401/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 402/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 403/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 404/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 405/1000\n",
      "Train loss: 0.000228, Val loss: 0.000241\n",
      "[INFO] EPOCH: 406/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 407/1000\n",
      "Train loss: 0.000228, Val loss: 0.000237\n",
      "[INFO] EPOCH: 408/1000\n",
      "Train loss: 0.000228, Val loss: 0.000237\n",
      "[INFO] EPOCH: 409/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 410/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 411/1000\n",
      "Train loss: 0.000227, Val loss: 0.000238\n",
      "[INFO] EPOCH: 412/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 413/1000\n",
      "Train loss: 0.000228, Val loss: 0.000236\n",
      "[INFO] EPOCH: 414/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 415/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 416/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 417/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 418/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 419/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 420/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 421/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 422/1000\n",
      "Train loss: 0.000227, Val loss: 0.000237\n",
      "[INFO] EPOCH: 423/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 424/1000\n",
      "Train loss: 0.000227, Val loss: 0.000238\n",
      "[INFO] EPOCH: 425/1000\n",
      "Train loss: 0.000227, Val loss: 0.000236\n",
      "[INFO] EPOCH: 426/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 427/1000\n",
      "Train loss: 0.000228, Val loss: 0.000236\n",
      "[INFO] EPOCH: 428/1000\n",
      "Train loss: 0.000227, Val loss: 0.000238\n",
      "[INFO] EPOCH: 429/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 430/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 431/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 432/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 433/1000\n",
      "Train loss: 0.000228, Val loss: 0.000237\n",
      "[INFO] EPOCH: 434/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 435/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 436/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 437/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 438/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 439/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 440/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 441/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 442/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 443/1000\n",
      "Train loss: 0.000228, Val loss: 0.000237\n",
      "[INFO] EPOCH: 444/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 445/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 446/1000\n",
      "Train loss: 0.000227, Val loss: 0.000236\n",
      "[INFO] EPOCH: 447/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 448/1000\n",
      "Train loss: 0.000227, Val loss: 0.000237\n",
      "[INFO] EPOCH: 449/1000\n",
      "Train loss: 0.000229, Val loss: 0.000237\n",
      "[INFO] EPOCH: 450/1000\n",
      "Train loss: 0.000227, Val loss: 0.000240\n",
      "[INFO] EPOCH: 451/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 452/1000\n",
      "Train loss: 0.000227, Val loss: 0.000243\n",
      "[INFO] EPOCH: 453/1000\n",
      "Train loss: 0.000228, Val loss: 0.000238\n",
      "[INFO] EPOCH: 454/1000\n",
      "Train loss: 0.000227, Val loss: 0.000236\n",
      "[INFO] EPOCH: 455/1000\n",
      "Train loss: 0.000229, Val loss: 0.000234\n",
      "[INFO] EPOCH: 456/1000\n",
      "Train loss: 0.000227, Val loss: 0.000237\n",
      "[INFO] EPOCH: 457/1000\n",
      "Train loss: 0.000228, Val loss: 0.000239\n",
      "[INFO] EPOCH: 458/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 459/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 460/1000\n",
      "Train loss: 0.000227, Val loss: 0.000238\n",
      "[INFO] EPOCH: 461/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 462/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 463/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 464/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 465/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 466/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 467/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 468/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 469/1000\n",
      "Train loss: 0.000227, Val loss: 0.000237\n",
      "[INFO] EPOCH: 470/1000\n",
      "Train loss: 0.000229, Val loss: 0.000234\n",
      "[INFO] EPOCH: 471/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 472/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 473/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 474/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 475/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 476/1000\n",
      "Train loss: 0.000228, Val loss: 0.000237\n",
      "[INFO] EPOCH: 477/1000\n",
      "Train loss: 0.000229, Val loss: 0.000240\n",
      "[INFO] EPOCH: 478/1000\n",
      "Train loss: 0.000228, Val loss: 0.000242\n",
      "[INFO] EPOCH: 479/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 480/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 481/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 482/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 483/1000\n",
      "Train loss: 0.000227, Val loss: 0.000239\n",
      "[INFO] EPOCH: 484/1000\n",
      "Train loss: 0.000228, Val loss: 0.000236\n",
      "[INFO] EPOCH: 485/1000\n",
      "Train loss: 0.000229, Val loss: 0.000234\n",
      "[INFO] EPOCH: 486/1000\n",
      "Train loss: 0.000228, Val loss: 0.000238\n",
      "[INFO] EPOCH: 487/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 488/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 489/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 490/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 491/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 492/1000\n",
      "Train loss: 0.000228, Val loss: 0.000240\n",
      "[INFO] EPOCH: 493/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 494/1000\n",
      "Train loss: 0.000228, Val loss: 0.000239\n",
      "[INFO] EPOCH: 495/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 496/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 497/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 498/1000\n",
      "Train loss: 0.000227, Val loss: 0.000237\n",
      "[INFO] EPOCH: 499/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 500/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 501/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 502/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 503/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 504/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 505/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 506/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 507/1000\n",
      "Train loss: 0.000227, Val loss: 0.000236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 508/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 509/1000\n",
      "Train loss: 0.000228, Val loss: 0.000237\n",
      "[INFO] EPOCH: 510/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 511/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 512/1000\n",
      "Train loss: 0.000227, Val loss: 0.000238\n",
      "[INFO] EPOCH: 513/1000\n",
      "Train loss: 0.000227, Val loss: 0.000239\n",
      "[INFO] EPOCH: 514/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 515/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 516/1000\n",
      "Train loss: 0.000228, Val loss: 0.000244\n",
      "[INFO] EPOCH: 517/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 518/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 519/1000\n",
      "Train loss: 0.000227, Val loss: 0.000237\n",
      "[INFO] EPOCH: 520/1000\n",
      "Train loss: 0.000227, Val loss: 0.000238\n",
      "[INFO] EPOCH: 521/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 522/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 523/1000\n",
      "Train loss: 0.000227, Val loss: 0.000236\n",
      "[INFO] EPOCH: 524/1000\n",
      "Train loss: 0.000228, Val loss: 0.000236\n",
      "[INFO] EPOCH: 525/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 526/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 527/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 528/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 529/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 530/1000\n",
      "Train loss: 0.000228, Val loss: 0.000237\n",
      "[INFO] EPOCH: 531/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 532/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 533/1000\n",
      "Train loss: 0.000228, Val loss: 0.000239\n",
      "[INFO] EPOCH: 534/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 535/1000\n",
      "Train loss: 0.000227, Val loss: 0.000236\n",
      "[INFO] EPOCH: 536/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 537/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 538/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 539/1000\n",
      "Train loss: 0.000228, Val loss: 0.000236\n",
      "[INFO] EPOCH: 540/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 541/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 542/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 543/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 544/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 545/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 546/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 547/1000\n",
      "Train loss: 0.000227, Val loss: 0.000236\n",
      "[INFO] EPOCH: 548/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 549/1000\n",
      "Train loss: 0.000227, Val loss: 0.000240\n",
      "[INFO] EPOCH: 550/1000\n",
      "Train loss: 0.000228, Val loss: 0.000238\n",
      "[INFO] EPOCH: 551/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 552/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 553/1000\n",
      "Train loss: 0.000228, Val loss: 0.000238\n",
      "[INFO] EPOCH: 554/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 555/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 556/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 557/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 558/1000\n",
      "Train loss: 0.000227, Val loss: 0.000239\n",
      "[INFO] EPOCH: 559/1000\n",
      "Train loss: 0.000229, Val loss: 0.000236\n",
      "[INFO] EPOCH: 560/1000\n",
      "Train loss: 0.000228, Val loss: 0.000238\n",
      "[INFO] EPOCH: 561/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 562/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 563/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 564/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 565/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 566/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 567/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 568/1000\n",
      "Train loss: 0.000228, Val loss: 0.000237\n",
      "[INFO] EPOCH: 569/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 570/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 571/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 572/1000\n",
      "Train loss: 0.000229, Val loss: 0.000235\n",
      "[INFO] EPOCH: 573/1000\n",
      "Train loss: 0.000227, Val loss: 0.000236\n",
      "[INFO] EPOCH: 574/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 575/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 576/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 577/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 578/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 579/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 580/1000\n",
      "Train loss: 0.000228, Val loss: 0.000236\n",
      "[INFO] EPOCH: 581/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 582/1000\n",
      "Train loss: 0.000228, Val loss: 0.000236\n",
      "[INFO] EPOCH: 583/1000\n",
      "Train loss: 0.000227, Val loss: 0.000238\n",
      "[INFO] EPOCH: 584/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 585/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 586/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 587/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 588/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 589/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 590/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 591/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 592/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 593/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 594/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 595/1000\n",
      "Train loss: 0.000227, Val loss: 0.000236\n",
      "[INFO] EPOCH: 596/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 597/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 598/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 599/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 600/1000\n",
      "Train loss: 0.000227, Val loss: 0.000236\n",
      "[INFO] EPOCH: 601/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 602/1000\n",
      "Train loss: 0.000228, Val loss: 0.000237\n",
      "[INFO] EPOCH: 603/1000\n",
      "Train loss: 0.000228, Val loss: 0.000236\n",
      "[INFO] EPOCH: 604/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 605/1000\n",
      "Train loss: 0.000227, Val loss: 0.000237\n",
      "[INFO] EPOCH: 606/1000\n",
      "Train loss: 0.000228, Val loss: 0.000237\n",
      "[INFO] EPOCH: 607/1000\n",
      "Train loss: 0.000228, Val loss: 0.000245\n",
      "[INFO] EPOCH: 608/1000\n",
      "Train loss: 0.000229, Val loss: 0.000239\n",
      "[INFO] EPOCH: 609/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 610/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 611/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 612/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 613/1000\n",
      "Train loss: 0.000227, Val loss: 0.000236\n",
      "[INFO] EPOCH: 614/1000\n",
      "Train loss: 0.000228, Val loss: 0.000239\n",
      "[INFO] EPOCH: 615/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 616/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 617/1000\n",
      "Train loss: 0.000227, Val loss: 0.000240\n",
      "[INFO] EPOCH: 618/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 619/1000\n",
      "Train loss: 0.000228, Val loss: 0.000238\n",
      "[INFO] EPOCH: 620/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 621/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 622/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 623/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 624/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 625/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 626/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 627/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 628/1000\n",
      "Train loss: 0.000227, Val loss: 0.000236\n",
      "[INFO] EPOCH: 629/1000\n",
      "Train loss: 0.000228, Val loss: 0.000236\n",
      "[INFO] EPOCH: 630/1000\n",
      "Train loss: 0.000227, Val loss: 0.000236\n",
      "[INFO] EPOCH: 631/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 632/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 633/1000\n",
      "Train loss: 0.000228, Val loss: 0.000238\n",
      "[INFO] EPOCH: 634/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 635/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 636/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 637/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 638/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 639/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 640/1000\n",
      "Train loss: 0.000228, Val loss: 0.000241\n",
      "[INFO] EPOCH: 641/1000\n",
      "Train loss: 0.000228, Val loss: 0.000236\n",
      "[INFO] EPOCH: 642/1000\n",
      "Train loss: 0.000228, Val loss: 0.000237\n",
      "[INFO] EPOCH: 643/1000\n",
      "Train loss: 0.000228, Val loss: 0.000240\n",
      "[INFO] EPOCH: 644/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 645/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 646/1000\n",
      "Train loss: 0.000227, Val loss: 0.000236\n",
      "[INFO] EPOCH: 647/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 648/1000\n",
      "Train loss: 0.000227, Val loss: 0.000236\n",
      "[INFO] EPOCH: 649/1000\n",
      "Train loss: 0.000229, Val loss: 0.000236\n",
      "[INFO] EPOCH: 650/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 651/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 652/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 653/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 654/1000\n",
      "Train loss: 0.000228, Val loss: 0.000238\n",
      "[INFO] EPOCH: 655/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 656/1000\n",
      "Train loss: 0.000228, Val loss: 0.000238\n",
      "[INFO] EPOCH: 657/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 658/1000\n",
      "Train loss: 0.000228, Val loss: 0.000237\n",
      "[INFO] EPOCH: 659/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 660/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 661/1000\n",
      "Train loss: 0.000227, Val loss: 0.000239\n",
      "[INFO] EPOCH: 662/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 663/1000\n",
      "Train loss: 0.000228, Val loss: 0.000240\n",
      "[INFO] EPOCH: 664/1000\n",
      "Train loss: 0.000228, Val loss: 0.000236\n",
      "[INFO] EPOCH: 665/1000\n",
      "Train loss: 0.000228, Val loss: 0.000236\n",
      "[INFO] EPOCH: 666/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 667/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 668/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 669/1000\n",
      "Train loss: 0.000228, Val loss: 0.000237\n",
      "[INFO] EPOCH: 670/1000\n",
      "Train loss: 0.000227, Val loss: 0.000236\n",
      "[INFO] EPOCH: 671/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 672/1000\n",
      "Train loss: 0.000228, Val loss: 0.000242\n",
      "[INFO] EPOCH: 673/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 674/1000\n",
      "Train loss: 0.000229, Val loss: 0.000234\n",
      "[INFO] EPOCH: 675/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 676/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 677/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 678/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 679/1000\n",
      "Train loss: 0.000228, Val loss: 0.000237\n",
      "[INFO] EPOCH: 680/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 681/1000\n",
      "Train loss: 0.000227, Val loss: 0.000243\n",
      "[INFO] EPOCH: 682/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 683/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 684/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 685/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 686/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 687/1000\n",
      "Train loss: 0.000227, Val loss: 0.000236\n",
      "[INFO] EPOCH: 688/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 689/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 690/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 691/1000\n",
      "Train loss: 0.000229, Val loss: 0.000245\n",
      "[INFO] EPOCH: 692/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 693/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 694/1000\n",
      "Train loss: 0.000228, Val loss: 0.000236\n",
      "[INFO] EPOCH: 695/1000\n",
      "Train loss: 0.000227, Val loss: 0.000237\n",
      "[INFO] EPOCH: 696/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 697/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 698/1000\n",
      "Train loss: 0.000228, Val loss: 0.000239\n",
      "[INFO] EPOCH: 699/1000\n",
      "Train loss: 0.000229, Val loss: 0.000235\n",
      "[INFO] EPOCH: 700/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 701/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 702/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 703/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 704/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 705/1000\n",
      "Train loss: 0.000229, Val loss: 0.000237\n",
      "[INFO] EPOCH: 706/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 707/1000\n",
      "Train loss: 0.000227, Val loss: 0.000236\n",
      "[INFO] EPOCH: 708/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 709/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 710/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 711/1000\n",
      "Train loss: 0.000228, Val loss: 0.000236\n",
      "[INFO] EPOCH: 712/1000\n",
      "Train loss: 0.000227, Val loss: 0.000236\n",
      "[INFO] EPOCH: 713/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 714/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 715/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 716/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 717/1000\n",
      "Train loss: 0.000229, Val loss: 0.000236\n",
      "[INFO] EPOCH: 718/1000\n",
      "Train loss: 0.000228, Val loss: 0.000236\n",
      "[INFO] EPOCH: 719/1000\n",
      "Train loss: 0.000229, Val loss: 0.000235\n",
      "[INFO] EPOCH: 720/1000\n",
      "Train loss: 0.000227, Val loss: 0.000242\n",
      "[INFO] EPOCH: 721/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 722/1000\n",
      "Train loss: 0.000227, Val loss: 0.000238\n",
      "[INFO] EPOCH: 723/1000\n",
      "Train loss: 0.000229, Val loss: 0.000234\n",
      "[INFO] EPOCH: 724/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 725/1000\n",
      "Train loss: 0.000227, Val loss: 0.000239\n",
      "[INFO] EPOCH: 726/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 727/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 728/1000\n",
      "Train loss: 0.000227, Val loss: 0.000239\n",
      "[INFO] EPOCH: 729/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 730/1000\n",
      "Train loss: 0.000227, Val loss: 0.000237\n",
      "[INFO] EPOCH: 731/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 732/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 733/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 734/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 735/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 736/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 737/1000\n",
      "Train loss: 0.000227, Val loss: 0.000239\n",
      "[INFO] EPOCH: 738/1000\n",
      "Train loss: 0.000228, Val loss: 0.000237\n",
      "[INFO] EPOCH: 739/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 740/1000\n",
      "Train loss: 0.000228, Val loss: 0.000238\n",
      "[INFO] EPOCH: 741/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 742/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 743/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 744/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 745/1000\n",
      "Train loss: 0.000228, Val loss: 0.000238\n",
      "[INFO] EPOCH: 746/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 747/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 748/1000\n",
      "Train loss: 0.000228, Val loss: 0.000238\n",
      "[INFO] EPOCH: 749/1000\n",
      "Train loss: 0.000227, Val loss: 0.000240\n",
      "[INFO] EPOCH: 750/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 751/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 752/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 753/1000\n",
      "Train loss: 0.000229, Val loss: 0.000234\n",
      "[INFO] EPOCH: 754/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 755/1000\n",
      "Train loss: 0.000228, Val loss: 0.000237\n",
      "[INFO] EPOCH: 756/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 757/1000\n",
      "Train loss: 0.000229, Val loss: 0.000235\n",
      "[INFO] EPOCH: 758/1000\n",
      "Train loss: 0.000227, Val loss: 0.000240\n",
      "[INFO] EPOCH: 759/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 760/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 761/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 762/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 763/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 764/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 765/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 766/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 767/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 768/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 769/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 770/1000\n",
      "Train loss: 0.000227, Val loss: 0.000239\n",
      "[INFO] EPOCH: 771/1000\n",
      "Train loss: 0.000227, Val loss: 0.000241\n",
      "[INFO] EPOCH: 772/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 773/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 774/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 775/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 776/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 777/1000\n",
      "Train loss: 0.000226, Val loss: 0.000242\n",
      "[INFO] EPOCH: 778/1000\n",
      "Train loss: 0.000229, Val loss: 0.000236\n",
      "[INFO] EPOCH: 779/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 780/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 781/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 782/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 783/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 784/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 785/1000\n",
      "Train loss: 0.000227, Val loss: 0.000243\n",
      "[INFO] EPOCH: 786/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 787/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 788/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 789/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 790/1000\n",
      "Train loss: 0.000227, Val loss: 0.000237\n",
      "[INFO] EPOCH: 791/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 792/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 793/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 794/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 795/1000\n",
      "Train loss: 0.000227, Val loss: 0.000236\n",
      "[INFO] EPOCH: 796/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 797/1000\n",
      "Train loss: 0.000227, Val loss: 0.000240\n",
      "[INFO] EPOCH: 798/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 799/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 800/1000\n",
      "Train loss: 0.000227, Val loss: 0.000236\n",
      "[INFO] EPOCH: 801/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 802/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 803/1000\n",
      "Train loss: 0.000227, Val loss: 0.000237\n",
      "[INFO] EPOCH: 804/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 805/1000\n",
      "Train loss: 0.000228, Val loss: 0.000240\n",
      "[INFO] EPOCH: 806/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 807/1000\n",
      "Train loss: 0.000227, Val loss: 0.000241\n",
      "[INFO] EPOCH: 808/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 809/1000\n",
      "Train loss: 0.000227, Val loss: 0.000236\n",
      "[INFO] EPOCH: 810/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 811/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 812/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 813/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 814/1000\n",
      "Train loss: 0.000227, Val loss: 0.000236\n",
      "[INFO] EPOCH: 815/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 816/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 817/1000\n",
      "Train loss: 0.000227, Val loss: 0.000237\n",
      "[INFO] EPOCH: 818/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 819/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 820/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 821/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 822/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 823/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 824/1000\n",
      "Train loss: 0.000227, Val loss: 0.000239\n",
      "[INFO] EPOCH: 825/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 826/1000\n",
      "Train loss: 0.000228, Val loss: 0.000236\n",
      "[INFO] EPOCH: 827/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 828/1000\n",
      "Train loss: 0.000227, Val loss: 0.000239\n",
      "[INFO] EPOCH: 829/1000\n",
      "Train loss: 0.000227, Val loss: 0.000237\n",
      "[INFO] EPOCH: 830/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 831/1000\n",
      "Train loss: 0.000228, Val loss: 0.000238\n",
      "[INFO] EPOCH: 832/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 833/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 834/1000\n",
      "Train loss: 0.000227, Val loss: 0.000241\n",
      "[INFO] EPOCH: 835/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 836/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 837/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 838/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 839/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 840/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 841/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 842/1000\n",
      "Train loss: 0.000228, Val loss: 0.000238\n",
      "[INFO] EPOCH: 843/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 844/1000\n",
      "Train loss: 0.000227, Val loss: 0.000240\n",
      "[INFO] EPOCH: 845/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 846/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 847/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 848/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 849/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 850/1000\n",
      "Train loss: 0.000228, Val loss: 0.000239\n",
      "[INFO] EPOCH: 851/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 852/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 853/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 854/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 855/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 856/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 857/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 858/1000\n",
      "Train loss: 0.000228, Val loss: 0.000236\n",
      "[INFO] EPOCH: 859/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 860/1000\n",
      "Train loss: 0.000228, Val loss: 0.000236\n",
      "[INFO] EPOCH: 861/1000\n",
      "Train loss: 0.000228, Val loss: 0.000239\n",
      "[INFO] EPOCH: 862/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 863/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 864/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 865/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 866/1000\n",
      "Train loss: 0.000228, Val loss: 0.000240\n",
      "[INFO] EPOCH: 867/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 868/1000\n",
      "Train loss: 0.000228, Val loss: 0.000236\n",
      "[INFO] EPOCH: 869/1000\n",
      "Train loss: 0.000228, Val loss: 0.000239\n",
      "[INFO] EPOCH: 870/1000\n",
      "Train loss: 0.000228, Val loss: 0.000237\n",
      "[INFO] EPOCH: 871/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 872/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 873/1000\n",
      "Train loss: 0.000228, Val loss: 0.000237\n",
      "[INFO] EPOCH: 874/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 875/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 876/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 877/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 878/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 879/1000\n",
      "Train loss: 0.000228, Val loss: 0.000239\n",
      "[INFO] EPOCH: 880/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 881/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 882/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 883/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 884/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 885/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 886/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 887/1000\n",
      "Train loss: 0.000228, Val loss: 0.000236\n",
      "[INFO] EPOCH: 888/1000\n",
      "Train loss: 0.000229, Val loss: 0.000238\n",
      "[INFO] EPOCH: 889/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 890/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 891/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 892/1000\n",
      "Train loss: 0.000228, Val loss: 0.000236\n",
      "[INFO] EPOCH: 893/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 894/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 895/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 896/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 897/1000\n",
      "Train loss: 0.000228, Val loss: 0.000237\n",
      "[INFO] EPOCH: 898/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 899/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 900/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 901/1000\n",
      "Train loss: 0.000228, Val loss: 0.000242\n",
      "[INFO] EPOCH: 902/1000\n",
      "Train loss: 0.000228, Val loss: 0.000236\n",
      "[INFO] EPOCH: 903/1000\n",
      "Train loss: 0.000227, Val loss: 0.000238\n",
      "[INFO] EPOCH: 904/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 905/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 906/1000\n",
      "Train loss: 0.000227, Val loss: 0.000236\n",
      "[INFO] EPOCH: 907/1000\n",
      "Train loss: 0.000228, Val loss: 0.000236\n",
      "[INFO] EPOCH: 908/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 909/1000\n",
      "Train loss: 0.000229, Val loss: 0.000235\n",
      "[INFO] EPOCH: 910/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 911/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 912/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 913/1000\n",
      "Train loss: 0.000228, Val loss: 0.000237\n",
      "[INFO] EPOCH: 914/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 915/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 916/1000\n",
      "Train loss: 0.000227, Val loss: 0.000238\n",
      "[INFO] EPOCH: 917/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 918/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 919/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 920/1000\n",
      "Train loss: 0.000227, Val loss: 0.000244\n",
      "[INFO] EPOCH: 921/1000\n",
      "Train loss: 0.000227, Val loss: 0.000238\n",
      "[INFO] EPOCH: 922/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 923/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 924/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 925/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 926/1000\n",
      "Train loss: 0.000228, Val loss: 0.000243\n",
      "[INFO] EPOCH: 927/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 928/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 929/1000\n",
      "Train loss: 0.000228, Val loss: 0.000236\n",
      "[INFO] EPOCH: 930/1000\n",
      "Train loss: 0.000227, Val loss: 0.000238\n",
      "[INFO] EPOCH: 931/1000\n",
      "Train loss: 0.000228, Val loss: 0.000236\n",
      "[INFO] EPOCH: 932/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 933/1000\n",
      "Train loss: 0.000227, Val loss: 0.000236\n",
      "[INFO] EPOCH: 934/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 935/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 936/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 937/1000\n",
      "Train loss: 0.000228, Val loss: 0.000236\n",
      "[INFO] EPOCH: 938/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 939/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 940/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 941/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 942/1000\n",
      "Train loss: 0.000229, Val loss: 0.000235\n",
      "[INFO] EPOCH: 943/1000\n",
      "Train loss: 0.000227, Val loss: 0.000236\n",
      "[INFO] EPOCH: 944/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 945/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 946/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 947/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 948/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 949/1000\n",
      "Train loss: 0.000228, Val loss: 0.000236\n",
      "[INFO] EPOCH: 950/1000\n",
      "Train loss: 0.000228, Val loss: 0.000237\n",
      "[INFO] EPOCH: 951/1000\n",
      "Train loss: 0.000228, Val loss: 0.000239\n",
      "[INFO] EPOCH: 952/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 953/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 954/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 955/1000\n",
      "Train loss: 0.000228, Val loss: 0.000237\n",
      "[INFO] EPOCH: 956/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 957/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 958/1000\n",
      "Train loss: 0.000227, Val loss: 0.000239\n",
      "[INFO] EPOCH: 959/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 960/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 961/1000\n",
      "Train loss: 0.000228, Val loss: 0.000236\n",
      "[INFO] EPOCH: 962/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 963/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 964/1000\n",
      "Train loss: 0.000228, Val loss: 0.000236\n",
      "[INFO] EPOCH: 965/1000\n",
      "Train loss: 0.000227, Val loss: 0.000238\n",
      "[INFO] EPOCH: 966/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 967/1000\n",
      "Train loss: 0.000227, Val loss: 0.000241\n",
      "[INFO] EPOCH: 968/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 969/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 970/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 971/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 972/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 973/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 974/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 975/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 976/1000\n",
      "Train loss: 0.000228, Val loss: 0.000236\n",
      "[INFO] EPOCH: 977/1000\n",
      "Train loss: 0.000227, Val loss: 0.000241\n",
      "[INFO] EPOCH: 978/1000\n",
      "Train loss: 0.000227, Val loss: 0.000239\n",
      "[INFO] EPOCH: 979/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 980/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 981/1000\n",
      "Train loss: 0.000229, Val loss: 0.000234\n",
      "[INFO] EPOCH: 982/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 983/1000\n",
      "Train loss: 0.000227, Val loss: 0.000237\n",
      "[INFO] EPOCH: 984/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 985/1000\n",
      "Train loss: 0.000229, Val loss: 0.000238\n",
      "[INFO] EPOCH: 986/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 987/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 988/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 989/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 990/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 991/1000\n",
      "Train loss: 0.000227, Val loss: 0.000240\n",
      "[INFO] EPOCH: 992/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 993/1000\n",
      "Train loss: 0.000227, Val loss: 0.000234\n",
      "[INFO] EPOCH: 994/1000\n",
      "Train loss: 0.000228, Val loss: 0.000236\n",
      "[INFO] EPOCH: 995/1000\n",
      "Train loss: 0.000228, Val loss: 0.000240\n",
      "[INFO] EPOCH: 996/1000\n",
      "Train loss: 0.000228, Val loss: 0.000235\n",
      "[INFO] EPOCH: 997/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 998/1000\n",
      "Train loss: 0.000227, Val loss: 0.000235\n",
      "[INFO] EPOCH: 999/1000\n",
      "Train loss: 0.000228, Val loss: 0.000234\n",
      "[INFO] EPOCH: 1000/1000\n",
      "Train loss: 0.000227, Val loss: 0.000237\n",
      "[INFO] total time taken to train the model:         4968.35s\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 1000\n",
    "print(\"[INFO] training the network...\")\n",
    "\n",
    "\n",
    "startTime = time.time()\n",
    "# loop over our epochs\n",
    "for e in range(0, EPOCHS):\n",
    "    # set the model in training mode\n",
    "    model.train()\n",
    "    # initialize the total training and validation loss\n",
    "    totalTrainLoss = 0\n",
    "    totalValLoss = 0\n",
    "   \n",
    "    # loop over the training set\n",
    "    for id_batch, (x, y) in enumerate(trainDataLoader):\n",
    "#         print(id_batch)\n",
    "#         print(y)\n",
    "        # send the input to the device\n",
    "        (x, y) = (x.to(device), y.to(device))\n",
    "#         print(y.size(), y.type())\n",
    "#         print(x.size(), x.type())\n",
    "#         print(x)\n",
    "        # perform a forward pass and calculate the training loss\n",
    "        pred = model(x)\n",
    "#         print(pred)\n",
    "\n",
    "        loss = lossMSE(pred, y)\n",
    "        # zero out the gradients, perform the backpropagation step,\n",
    "        # and update the weights\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        # add the loss to the total training loss so far and\n",
    "        totalTrainLoss += loss\n",
    "    # switch off autograd for evaluation\n",
    "    with torch.no_grad():\n",
    "        # set the model in evaluation mode\n",
    "        model.eval()\n",
    "        # loop over the validation set\n",
    "        for (x, y) in valDataLoader:\n",
    "            # send the input to the device\n",
    "            (x, y) = (x.to(device), y.to(device))\n",
    "            # make the predictions and calculate the validation loss\n",
    "            pred = model(x)\n",
    "            totalValLoss += lossMSE(pred, y)\n",
    "\n",
    "    # calculate the average training and validation loss\n",
    "    avgTrainLoss = totalTrainLoss / trainSteps\n",
    "    avgValLoss = totalValLoss / valSteps\n",
    "\n",
    "    # update our training history\n",
    "    H[\"train_loss\"].append(avgTrainLoss.cpu().detach().numpy())\n",
    "    H[\"val_loss\"].append(avgValLoss.cpu().detach().numpy())\n",
    "    \n",
    "    # print the model training and validation information\n",
    "    print(\"[INFO] EPOCH: {}/{}\".format(e + 1, EPOCHS))\n",
    "    print(\"Train loss: {:.6f}, Val loss: {:.6f}\".format(\n",
    "        avgTrainLoss, avgValLoss))\n",
    "# finish measuring how long training took\n",
    "endTime = time.time()\n",
    "print(\"[INFO] total time taken to train the model: \\\n",
    "        {:.2f}s\".format(endTime - startTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "f6718aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] evaluating network...\n"
     ]
    }
   ],
   "source": [
    "# we can now evaluate the network on the test set\n",
    "print(\"[INFO] evaluating network...\")\n",
    "# turn off autograd for testing evaluation\n",
    "with torch.no_grad():\n",
    "    # set the model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # initialize a list to store our predictions\n",
    "    preds = []\n",
    "    # loop over the test set\n",
    "    for (x, y) in testDataLoader:\n",
    "        # send the input to the device\n",
    "        x = x.to(device)\n",
    "        # make the predictions and add them to the list\n",
    "        pred = model(x)\n",
    "        preds.extend(pred.argmax(axis=1).cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "6151c58a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fae31999580>"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the training loss and accuracy\n",
    "# plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(H[\"train_loss\"][120:], label=\"train_loss\")\n",
    "plt.plot(H[\"val_loss\"][120:], label=\"val_loss\")\n",
    "\n",
    "\n",
    "plt.title(\"Training Loss Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss MSE\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "# plt.savefig(args[\"plot\"])\n",
    "# serialize the model to disk\n",
    "# torch.save(model, args[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "dacf5763",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchmetrics'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [190]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MeanSquaredError\n\u001b[1;32m      2\u001b[0m mean_squared_error \u001b[38;5;241m=\u001b[39m MeanSquaredError()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# set the model in evaluation mode\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchmetrics'"
     ]
    }
   ],
   "source": [
    "from torchmetrics import MeanSquaredError\n",
    "mean_squared_error = MeanSquaredError()\n",
    "        \n",
    "with torch.no_grad():\n",
    "    # set the model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # initialize a list to store our predictions\n",
    "    preds = []\n",
    "    # loop over the test set\n",
    "    for (x, y) in trainDataLoader:\n",
    "        # send the input to the device\n",
    "        x = x.to(device)\n",
    "        # make the predictions and add them to the list\n",
    "        pred = model(x).flatten()\n",
    "        print(y)\n",
    "        print(pred)\n",
    "        print(mean_squared_error(pred, y))\n",
    "        \n",
    "        \n",
    "        \n",
    "#         print(y-pred)\n",
    "#         print(y.numpy(), (pred.flatten().cpu()).numpy())\n",
    "        plt.scatter(np.log(y.numpy()), (pred.flatten().cpu()).numpy(), color = 'black')\n",
    "        preds.extend(pred.argmax(axis=1).cpu().numpy())\n",
    "        \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae34c0c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4222b45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
