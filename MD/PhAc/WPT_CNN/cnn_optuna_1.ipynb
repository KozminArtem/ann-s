{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f51ba08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import flatten\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import torch.optim as optim\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch import nn\n",
    "from torch.nn import Module\n",
    "from torch.nn import Conv2d\n",
    "from torch.nn import Linear\n",
    "from torch.nn import MaxPool2d\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import BatchNorm2d\n",
    "from torchvision.transforms import ToTensor\n",
    "import glob\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import optuna\n",
    "from optuna.trial import TrialState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22df8760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000\n",
      "torch.Size([2, 64, 50])\n",
      "torch.DoubleTensor\n"
     ]
    }
   ],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.data = []\n",
    "\n",
    "        target_954 = np.load('data_segment/Y_natur_954.npz')\n",
    "        for j in range(3000):\n",
    "            self.data.append([\"data_segment/natur_X/954_\" + str(j) + '.npz',target_954['arr_0'][j]])\n",
    "#         print(self.data)\n",
    "\n",
    "        target_9_7 = np.load('data_segment/Y_natur_9_7.npz')\n",
    "#         print(target_9_7['arr_0'])\n",
    "        for j in range(3000):\n",
    "            self.data.append([\"data_segment/natur_X/9_7_\" + str(j) + '.npz',target_9_7['arr_0'][j]])\n",
    "\n",
    "        \n",
    "        target_Air = np.load('data_segment/Y_natur_Air.npz')\n",
    "#         print(target_Air['arr_0'])\n",
    "        for j in range(3000):\n",
    "            self.data.append([\"data_segment/natur_X/Air_\" + str(j) + '.npz',target_Air['arr_0'][j]])\n",
    "        \n",
    "        temp = np.load('data_segment/natur_X/954_0.npz')\n",
    "        self.img_dim = temp['arr_0'].shape  \n",
    "#         print(self.img_dim)\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)    \n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, target = self.data[idx]\n",
    "        \n",
    "        img = np.load(img_path)\n",
    "        img_tensor = torch.from_numpy(img['arr_0'])\n",
    "        \n",
    "        target_tensor = torch.tensor(target)\n",
    "#         print(target_tensor, target)\n",
    "        \n",
    "#         img_tensor = img_tensor.permute(2, 0, 1)\n",
    "#         class_id = torch.tensor([class_id])\n",
    "        \n",
    "        return img_tensor, target_tensor\n",
    "\n",
    "    \n",
    "data = CustomDataset()\n",
    "print(len(data))\n",
    "\n",
    "print(data[1][0].size())\n",
    "print(data[1][0].type())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "254b538a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6300\n",
      "1350\n",
      "1350\n"
     ]
    }
   ],
   "source": [
    "Train_split = 0.7\n",
    "Val_split = 0.15\n",
    "Test_split = 0.15\n",
    "(trainData, valData, testData) = random_split(data, [Train_split, Val_split, Test_split],\\\n",
    "                                              generator=torch.Generator().manual_seed(42))\n",
    "print(len(trainData))\n",
    "print(len(valData))\n",
    "print(len(testData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8f1d886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196\n",
      "42\n",
      "42\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "trainDataLoader = DataLoader(trainData, shuffle=True, batch_size=BATCH_SIZE)\n",
    "valDataLoader = DataLoader(valData, batch_size=BATCH_SIZE)\n",
    "testDataLoader = DataLoader(testData, batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "\n",
    "# print(testDataLoader.dataset[0])\n",
    "\n",
    "# calculate steps per epoch for training and validation set\n",
    "trainSteps = len(trainDataLoader.dataset) // BATCH_SIZE\n",
    "print(trainSteps)\n",
    "\n",
    "valSteps = len(valDataLoader.dataset) // BATCH_SIZE\n",
    "print(valSteps)\n",
    "\n",
    "testSteps = len(testDataLoader.dataset) // BATCH_SIZE\n",
    "print(testSteps)\n",
    "\n",
    "\n",
    "# print(trainDataLoader.dataset)\n",
    "# print(trainDataLoader.dataset[1])\n",
    "# print(trainDataLoader.dataset[1][0].type())\n",
    "# print(trainDataLoader.dataset[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7687455",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(Module):\n",
    "    def __init__(self):\n",
    "        # call the parent constructor\n",
    "        super(LeNet, self).__init__()\n",
    "\n",
    "        self.norm1 = BatchNorm2d(2)\n",
    "\n",
    "        # initialize first set of CONV => RELU => POOL layers\n",
    "        self.conv1 = Conv2d(in_channels = 2, out_channels = 8,\\\n",
    "                            kernel_size = (16, 20), stride = (16,10))#16 - height, 20 - width \n",
    "        self.relu1 = ReLU()\n",
    "        self.maxpool1 = MaxPool2d(kernel_size=(2, 2), stride=(2, 2)) #stride = step\n",
    "        \n",
    "#         self.norm2 = BatchNorm2d(8)\n",
    "        \n",
    "        # initialize first (and only) set of FC => RELU layers\n",
    "        self.fc1 = Linear(in_features=32, out_features=8)\n",
    "        self.relu2 = ReLU()\n",
    "        self.fc2 = Linear(in_features=8, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # pass the input through our first set of CONV => RELU =>\n",
    "        # POOL layers\n",
    "#         print(x)\n",
    "        x = self.norm1(x)\n",
    "    \n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        \n",
    "        x = flatten(x, 1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.relu2(x)\n",
    "\n",
    "        output = self.fc2(x)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f3fb73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# set the device we will be using to train the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20256705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] initializing the LeNet model...\n",
      "LeNet(\n",
      "  (norm1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv1): Conv2d(2, 8, kernel_size=(16, 20), stride=(16, 10))\n",
      "  (relu1): ReLU()\n",
      "  (maxpool1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=32, out_features=8, bias=True)\n",
      "  (relu2): ReLU()\n",
      "  (fc2): Linear(in_features=8, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define training hyperparameters\n",
    "INIT_LR = 0.001\n",
    "\n",
    "# initialize the LeNet model\n",
    "print(\"[INFO] initializing the LeNet model...\")\n",
    "\n",
    "model = LeNet().to(device)\n",
    "model.type(torch.cuda.DoubleTensor)\n",
    "# model.type(torch.DoubleTensor)\n",
    "\n",
    "\n",
    "print(model)\n",
    "# initialize our optimizer and loss function\n",
    "opt = Adam(model.parameters(), lr=INIT_LR)\n",
    "\n",
    "scheduler = ReduceLROnPlateau(opt, 'min')\n",
    "\n",
    "\n",
    "\n",
    "lossMSE = nn.MSELoss()\n",
    "# lossMAE = nn.L1Loss()\n",
    "\n",
    "# initialize a dictionary to store training history\n",
    "H = {\"train_loss\": [],\"val_loss\": []}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d3da5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training the network...\n",
      "[INFO] EPOCH: 1/10\n",
      "Train loss: 0.036069, Val loss: 0.009152\n",
      "[INFO] EPOCH: 2/10\n",
      "Train loss: 0.007857, Val loss: 0.005744\n",
      "[INFO] EPOCH: 3/10\n",
      "Train loss: 0.008048, Val loss: 0.008242\n",
      "[INFO] EPOCH: 4/10\n",
      "Train loss: 0.008310, Val loss: 0.007903\n",
      "[INFO] EPOCH: 5/10\n",
      "Train loss: 0.006707, Val loss: 0.006259\n",
      "[INFO] EPOCH: 6/10\n",
      "Train loss: 0.009982, Val loss: 0.006280\n",
      "[INFO] EPOCH: 7/10\n",
      "Train loss: 0.004833, Val loss: 0.004316\n",
      "[INFO] EPOCH: 8/10\n",
      "Train loss: 0.004897, Val loss: 0.003507\n",
      "[INFO] EPOCH: 9/10\n",
      "Train loss: 0.003968, Val loss: 0.003489\n",
      "[INFO] EPOCH: 10/10\n",
      "Train loss: 0.004038, Val loss: 0.003111\n",
      "[INFO] total time taken to train the model:         50.24s\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "print(\"[INFO] training the network...\")\n",
    "\n",
    "\n",
    "startTime = time.time()\n",
    "# loop over our epochs\n",
    "for e in range(0, EPOCHS):\n",
    "    # set the model in training mode\n",
    "    model.train()\n",
    "    # initialize the total training and validation loss\n",
    "    totalTrainLoss = 0\n",
    "    totalValLoss = 0\n",
    "   \n",
    "    # loop over the training set\n",
    "    for id_batch, (x, y) in enumerate(trainDataLoader):\n",
    "#         print(id_batch)\n",
    "#         print(y)\n",
    "        # send the input to the device\n",
    "        (x, y) = (x.to(device), y.to(device))\n",
    "#         print(y.size(), y.type())\n",
    "#         print(x.size(), x.type())\n",
    "#         print(x)\n",
    "        # perform a forward pass and calculate the training loss\n",
    "        pred = model(x)\n",
    "#         print(pred)\n",
    "        y = torch.reshape(y,(len(y),1))\n",
    "\n",
    "        loss = lossMSE(pred, y)\n",
    "#         loss = lossMAE(pred, y)\n",
    "        \n",
    "        # zero out the gradients, perform the backpropagation step,\n",
    "        # and update the weights\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        # add the loss to the total training loss so far and\n",
    "        totalTrainLoss += loss\n",
    "    # switch off autograd for evaluation\n",
    "    with torch.no_grad():\n",
    "        # set the model in evaluation mode\n",
    "        model.eval()\n",
    "        # loop over the validation set\n",
    "        for (x, y) in valDataLoader:\n",
    "            # send the input to the device\n",
    "            (x, y) = (x.to(device), y.to(device))\n",
    "            # make the predictions and calculate the validation loss\n",
    "            pred = model(x)\n",
    "            y = torch.reshape(y,(len(y),1))\n",
    "            totalValLoss += lossMSE(pred, y)\n",
    "#             totalValLoss += lossMAE(pred, y)\n",
    "    \n",
    "    scheduler.step(totalValLoss)\n",
    "    \n",
    "    # calculate the average training and validation loss\n",
    "    avgTrainLoss = totalTrainLoss / trainSteps\n",
    "    avgValLoss = totalValLoss / valSteps\n",
    "\n",
    "    # update our training history\n",
    "    H[\"train_loss\"].append(avgTrainLoss.cpu().detach().numpy())\n",
    "    H[\"val_loss\"].append(avgValLoss.cpu().detach().numpy())\n",
    "    \n",
    "    # print the model training and validation information\n",
    "    print(\"[INFO] EPOCH: {}/{}\".format(e + 1, EPOCHS))\n",
    "    print(\"Train loss: {:.6f}, Val loss: {:.6f}\".format(\n",
    "        torch.sqrt(avgTrainLoss), torch.sqrt(avgValLoss)))\n",
    "# finish measuring how long training took\n",
    "endTime = time.time()\n",
    "print(\"[INFO] total time taken to train the model: \\\n",
    "        {:.2f}s\".format(endTime - startTime))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb16310",
   "metadata": {},
   "source": [
    "$$\\bf OPTUNA$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09d281dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(Module):\n",
    "    def __init__(self, trial):\n",
    "        # call the parent constructor\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        self.norm1 = BatchNorm2d(2)\n",
    "        \n",
    "        out_ch = trial.suggest_int(\"out_ch\", 2, 16, log = True)\n",
    "        self.conv1 = Conv2d(in_channels = 2, out_channels = out_ch,\\\n",
    "                            kernel_size = (16, 20), stride = (16,10))#16 - height, 20 - width \n",
    "        \n",
    "        activ_1 = trial.suggest_categorical(\"activ_1\", [nn.ELU(),nn.LogSigmoid(), nn.ReLU(), nn.Tanh()])\n",
    "        self.act1 = activ_1\n",
    "        \n",
    "        self.maxpool1 = MaxPool2d(kernel_size=(2, 2), stride=(2, 2)) #stride = step\n",
    "         \n",
    "        # initialize first (and only) set of FC => RELU layers\n",
    "        \n",
    "        n_neur = trial.suggest_int(\"n_neur\",2,128,log = True)\n",
    "        self.fc1 = Linear(in_features = 4*out_ch, out_features = n_neur)\n",
    "        \n",
    "        \n",
    "        activ_2 = trial.suggest_categorical(\"activ_2\", [nn.ELU(),nn.LogSigmoid(), nn.ReLU(), nn.Tanh()])\n",
    "        self.act2 = activ_2\n",
    "        \n",
    "        self.fc2 = Linear(in_features = n_neur, out_features = 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # pass the input through our first set of CONV => RELU =>\n",
    "        # POOL layers\n",
    "#         print(x)\n",
    "        x = self.norm1(x)\n",
    "    \n",
    "        x = self.conv1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        \n",
    "        x = flatten(x, 1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.act2(x)\n",
    "        output = self.fc2(x)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fcabbce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "# set the device we will be using to train the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    # Generate the model.\n",
    "    model = ConvNet(trial).to(device)\n",
    "    model.type(torch.cuda.DoubleTensor)\n",
    "\n",
    "    # Generate the optimizers.\n",
    "\n",
    "    # try RMSprop and SGD\n",
    "    '''\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"RMSprop\", \"SGD\"])\n",
    "    momentum = trial.suggest_float(\"momentum\", 0.0, 1.0)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
    "    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr,momentum=momentum)\n",
    "    '''\n",
    "    #try Adam, AdaDelta adn Adagrad\n",
    "    \n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"Adadelta\",\"Adagrad\"])\n",
    "    \n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
    "    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
    "    \n",
    "    \n",
    "#     scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
    "    \n",
    "    batch_size=trial.suggest_int(\"batch_size\", 4, 256, log = True)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    \n",
    "    trainDataLoader = DataLoader(trainData, shuffle = True, batch_size = batch_size)\n",
    "    valDataLoader = DataLoader(valData, batch_size = batch_size)\n",
    "#     testDataLoader = DataLoader(testData, batch_size = batch_size)\n",
    "    \n",
    "    trainSteps = len(trainDataLoader.dataset) // batch_size\n",
    "    valSteps = len(valDataLoader.dataset) // batch_size\n",
    "    \n",
    "    # Training of the model.\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "       \n",
    "        totalTrainLoss = 0\n",
    "        totalValLoss = 0\n",
    "    \n",
    "        for id_batch, (x, y) in enumerate(trainDataLoader):\n",
    "            (x, y) = (x.to(device), y.to(device))\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            pred = model(x)\n",
    "            loss = criterion(pred, torch.reshape(y,(len(y),1)))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            totalTrainLoss += loss    \n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for (x, y) in valDataLoader:\n",
    "                # send the input to the device\n",
    "                (x, y) = (x.to(device), y.to(device))\n",
    "                # make the predictions and calculate the validation loss\n",
    "                pred = model(x)\n",
    "                totalValLoss += criterion(pred, torch.reshape(y,(len(y),1)))\n",
    "    #             totalValLoss += lossMAE(pred, y)\n",
    "        \n",
    "        avgTrainLoss = totalTrainLoss / trainSteps\n",
    "        avgValLoss = totalValLoss / valSteps\n",
    "        \n",
    "        \n",
    "        trial.report(torch.sqrt(avgValLoss), epoch)\n",
    "\n",
    "        # Handle pruning based on the intermediate value.\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return avgValLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df4a3017",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-12-06 13:55:33,724]\u001b[0m A new study created in memory with name: no-name-1acdee21-3af7-4c86-98b9-1423d0d34d1d\u001b[0m\n",
      "/home/kozminartem/.local/lib/python3.8/site-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains ELU(alpha=1.0) which is of type ELU.\n",
      "  warnings.warn(message)\n",
      "/home/kozminartem/.local/lib/python3.8/site-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains LogSigmoid() which is of type LogSigmoid.\n",
      "  warnings.warn(message)\n",
      "/home/kozminartem/.local/lib/python3.8/site-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains ReLU() which is of type ReLU.\n",
      "  warnings.warn(message)\n",
      "/home/kozminartem/.local/lib/python3.8/site-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains Tanh() which is of type Tanh.\n",
      "  warnings.warn(message)\n",
      "\u001b[32m[I 2022-12-06 13:56:22,661]\u001b[0m Trial 0 finished with value: 0.005348349464970759 and parameters: {'out_ch': 7, 'activ_1': ELU(alpha=1.0), 'n_neur': 119, 'activ_2': LogSigmoid(), 'optimizer': 'Adagrad', 'lr': 7.0372061290327e-05, 'batch_size': 69}. Best is trial 0 with value: 0.005348349464970759.\u001b[0m\n",
      "\u001b[33m[W 2022-12-06 13:56:22,663]\u001b[0m Trial 1 failed because of the following error: ValueError('CategoricalDistribution does not support dynamic value space.')\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kozminartem/.local/lib/python3.8/site-packages/optuna/study/_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_5693/864985818.py\", line 9, in objective\n",
      "    model = ConvNet(trial).to(device)\n",
      "  File \"/tmp/ipykernel_5693/3758709316.py\", line 12, in __init__\n",
      "    activ_1 = trial.suggest_categorical(\"activ_1\", [nn.ELU(),nn.LogSigmoid(), nn.ReLU(), nn.Tanh()])\n",
      "  File \"/home/kozminartem/.local/lib/python3.8/site-packages/optuna/trial/_trial.py\", line 371, in suggest_categorical\n",
      "    return self._suggest(name, CategoricalDistribution(choices=choices))\n",
      "  File \"/home/kozminartem/.local/lib/python3.8/site-packages/optuna/trial/_trial.py\", line 600, in _suggest\n",
      "    storage.set_trial_param(trial_id, name, param_value_in_internal_repr, distribution)\n",
      "  File \"/home/kozminartem/.local/lib/python3.8/site-packages/optuna/storages/_in_memory.py\", line 213, in set_trial_param\n",
      "    distributions.check_distribution_compatibility(\n",
      "  File \"/home/kozminartem/.local/lib/python3.8/site-packages/optuna/distributions.py\", line 653, in check_distribution_compatibility\n",
      "    raise ValueError(\n",
      "ValueError: CategoricalDistribution does not support dynamic value space.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "CategoricalDistribution does not support dynamic value space.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m trial \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_trial\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msqrt_MSE: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(trial\u001b[38;5;241m.\u001b[39mvalue))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/optuna/study/study.py:419\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    317\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    324\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    325\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \n\u001b[1;32m    328\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 419\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as CircleCI).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/optuna/study/_optimize.py:234\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    230\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    233\u001b[0m ):\n\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/optuna/study/_optimize.py:196\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 196\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobjective\u001b[39m(trial):\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# Generate the model.\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mConvNet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     10\u001b[0m     model\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mDoubleTensor)\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Generate the optimizers.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# try RMSprop and SGD\u001b[39;00m\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36mConvNet.__init__\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m      8\u001b[0m out_ch \u001b[38;5;241m=\u001b[39m trial\u001b[38;5;241m.\u001b[39msuggest_int(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout_ch\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m16\u001b[39m, log \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1 \u001b[38;5;241m=\u001b[39m Conv2d(in_channels \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m, out_channels \u001b[38;5;241m=\u001b[39m out_ch,\\\n\u001b[1;32m     10\u001b[0m                     kernel_size \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m20\u001b[39m), stride \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m16\u001b[39m,\u001b[38;5;241m10\u001b[39m))\u001b[38;5;66;03m#16 - height, 20 - width \u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m activ_1 \u001b[38;5;241m=\u001b[39m \u001b[43mtrial\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuggest_categorical\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mactiv_1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mELU\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLogSigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mReLU\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTanh\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact1 \u001b[38;5;241m=\u001b[39m activ_1\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxpool1 \u001b[38;5;241m=\u001b[39m MaxPool2d(kernel_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m), stride\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m)) \u001b[38;5;66;03m#stride = step\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/optuna/trial/_trial.py:371\u001b[0m, in \u001b[0;36mTrial.suggest_categorical\u001b[0;34m(self, name, choices)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;124;03m\"\"\"Suggest a value for the categorical parameter.\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \n\u001b[1;32m    322\u001b[0m \u001b[38;5;124;03mThe value is sampled from ``choices``.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;124;03m    :ref:`configurations` tutorial describes more details and flexible usages.\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;66;03m# There is no need to call self._check_distribution because\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;66;03m# CategoricalDistribution does not support dynamic value space.\u001b[39;00m\n\u001b[0;32m--> 371\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_suggest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCategoricalDistribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchoices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchoices\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/optuna/trial/_trial.py:600\u001b[0m, in \u001b[0;36mTrial._suggest\u001b[0;34m(self, name, distribution)\u001b[0m\n\u001b[1;32m    595\u001b[0m         param_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstudy\u001b[38;5;241m.\u001b[39msampler\u001b[38;5;241m.\u001b[39msample_independent(\n\u001b[1;32m    596\u001b[0m             study, trial, name, distribution\n\u001b[1;32m    597\u001b[0m         )\n\u001b[1;32m    599\u001b[0m     param_value_in_internal_repr \u001b[38;5;241m=\u001b[39m distribution\u001b[38;5;241m.\u001b[39mto_internal_repr(param_value)\n\u001b[0;32m--> 600\u001b[0m     \u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_trial_param\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_value_in_internal_repr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistribution\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m param_value\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/optuna/storages/_in_memory.py:213\u001b[0m, in \u001b[0;36mInMemoryStorage.set_trial_param\u001b[0;34m(self, trial_id, param_name, param_value_internal, distribution)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;66;03m# Check param distribution compatibility with previous trial(s).\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m param_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_studies[study_id]\u001b[38;5;241m.\u001b[39mparam_distribution:\n\u001b[0;32m--> 213\u001b[0m     \u001b[43mdistributions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_distribution_compatibility\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_studies\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstudy_id\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_distribution\u001b[49m\u001b[43m[\u001b[49m\u001b[43mparam_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistribution\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;66;03m# Set param distribution.\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_studies[study_id]\u001b[38;5;241m.\u001b[39mparam_distribution[param_name] \u001b[38;5;241m=\u001b[39m distribution\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/optuna/distributions.py:653\u001b[0m, in \u001b[0;36mcheck_distribution_compatibility\u001b[0;34m(dist_old, dist_new)\u001b[0m\n\u001b[1;32m    651\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dist_old \u001b[38;5;241m!=\u001b[39m dist_new:\n\u001b[0;32m--> 653\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    654\u001b[0m         CategoricalDistribution\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m does not support dynamic value space.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    655\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: CategoricalDistribution does not support dynamic value space."
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "trial = study.best_trial\n",
    "\n",
    "print('sqrt_MSE: {}'.format(trial.value))\n",
    "print(\"Best hyperparameters: {}\".format(trial.params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56623961",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3becd998",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f229ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
