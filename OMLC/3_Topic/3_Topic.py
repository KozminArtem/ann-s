					### https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-3-classification-decision-trees-and-k-nearest-neighbors-8613c6b6d2cd
					#   Оpen Machine Learning Course. 
					#   Topic 3. Classification, Decision Trees and k Nearest Neighbors


# Привет! Это третья статья из нашей серии. Сегодня мы наконец добрались до машинного обучения. Это будет захватывающе!


import numpy as np
import pandas as pd

import warnings
warnings.simplefilter('ignore')

import matplotlib.pyplot as plt

import seaborn as sns
sns.set()

from pylab import rcParams
rcParams['figure.figsize'] = 5, 4 


# 1. Краткое содержание статьи
# 2. Вступление
# Древо решений
# Как построить дерево решений
# Алгоритм построения дерева
# Другие критерии качества для разделения в задачах классификации
# Как дерево решений работает с числовыми характеристиками
# Ключевые параметры дерева
# Класс DecisionTreeClassifier в Scikit-learn
# Дерево решений в задаче регрессии
# 3. Метод ближайших соседей
# Метод ближайших соседей в реальных приложениях
# Класс KNeighborsClassifier в Scikit-learn
# 4. Выбор параметров модели и перекрестная проверка
# 5. Примеры применения и сложные случаи
# Метод деревьев решений и ближайших соседей в задаче прогнозирования оттока клиентов
# Сложный случай для деревьев решений
# Деревья решений и k-NN в задаче распознавания рукописных цифр MNIST
# Сложный случай для метода ближайших соседей.
# 6. Плюсы и минусы деревьев решений и метода ближайших соседей


										# 1. Введение

# Прежде чем мы углубимся в материал для статьи на этой неделе, давайте поговорим о типе проблемы, 
# которую мы собираемся решить, и ее месте в захватывающей области машинного обучения. Книга Т. Митчелла 
# «Машинное обучение» (1997) дает классическое общее определение машинного обучения следующим образом:
# Считается, что компьютерная программа учится на опыте E (experience) в отношении некоторого класса задач T (tasks) 
# и показателя производительности P (performance), если ее производительность при выполнении задач в T, измеренная с помощью P, улучшается с опытом E.
# В различных настройках задачи T, P и E могут относиться к совершенно разным вещам. 

# Вот некоторые из самых популярных задач T в машинном обучении:
# класификация - отнесение экземпляра к одной из категорий по признакам;
# регрессия - прогноз числовой целевой характеристики на основе других характеристик экземпляра;
# кластеризация - определение разделов экземпляров на основе характеристик этих экземпляров, 
# чтобы члены внутри групп были более похожи друг на друга, чем члены других групп;
# обнаружение аномалий - поиск экземпляров, которые «сильно не похожи» на остальную часть выборки или на некоторую группу экземпляров;
# и многое другое.

# Хороший обзор представлен в главе «Основы машинного обучения» книги «Глубокое обучение» (Ян Гудфеллоу, Йошуа Бенжио, Аарон Курвиль, 2016).
# https://www.deeplearningbook.org/contents/ml.html

# Опыт E относится к данным (без них мы никуда не денемся). Алгоритмы машинного обучения можно разделить на те, 
# которые обучаются контролируемым и неконтролируемым образом. В задачах обучения без учителя есть набор,
# состоящий из экземпляров, описываемых набором функций. В задачах контролируемого обучения также есть
# целевая переменная, которую мы хотели бы предсказать, известная для каждого экземпляра в обучающей выборке.

# Пример
# Классификация и регрессия - это задачи обучения с учителем. Например, как кредитное учреждение, мы 
# можем захотеть спрогнозировать невозврат кредита на основе данных, собранных о наших клиентах. Здесь 
# опыт E - это доступные обучающие данные: набор экземпляров (клиентов), набор характеристик (таких как 
# возраст, зарплата, тип ссуды, прошлые дефолты по ссуде и т. Д.) Для каждого и целевая переменная (будь 
# то они объявили дефолт по кредиту). Эта целевая переменная является просто фактом невыполнения обязательств 
# по ссуде (1 или 0), поэтому помните, что это проблема (бинарной) классификации. Если бы вы вместо этого 
# предсказывали, на сколько времени просрочен платеж по кредиту, это превратилось бы в проблему регрессии.

# Наконец, третий термин, используемый в определении машинного обучения, - это показатель оценки производительности 
# алгоритма P. Такие показатели различаются для разных задач и алгоритмов, и мы будем обсуждать их при изучении 
# новых алгоритмов. А пока мы будем ссылаться на простую метрику для алгоритмов классификации - долю 
# правильных ответов - точность - на тестовом наборе.

# Давайте посмотрим на две задачи обучения с учителем: классификацию и регрессию.

							# 2. Дерево решений

# Обзор методов классификации и регрессии мы начнем с одного из самых популярных - дерева решений. 
# Деревья решений используются в повседневных решениях, а не только в машинном обучении. Блок-схемы
# на самом деле являются визуальным представлением деревьев решений. Например, Высшая школа экономики 
# издает информационные диаграммы, чтобы облегчить жизнь своим сотрудникам. Вот отрывок инструкций по 
# публикации статьи на портале учреждения.

# С точки зрения машинного обучения его можно рассматривать как простой классификатор, определяющий 
# подходящую форму публикации (книга, статья, глава книги, препринт, публикация в «Высшей школе экономики
# и СМИ») на основе контента. (книга, брошюра, статья), тип журнала, тип оригинальной публикации (научный 
# журнал, сборники) и т. д.

# Дерево решений часто является обобщением опыта экспертов, средством обмена знаниями о конкретном процессе. 
# Например, до внедрения масштабируемых алгоритмов машинного обучения задача кредитного скоринга в банковском 
# секторе решалась специалистами. Решение о предоставлении ссуды было принято на основе некоторых интуитивно 
# (или эмпирически) правил, которые можно было представить в виде дерева решений.

# В нашем следующем случае мы решаем задачу бинарной классификации (одобряем / отклоняем ссуду) 
# по признакам «Возраст», «Собственность», «Доход» и «Образование».

# Дерево решений как алгоритм машинного обучения по сути то же самое, что и диаграмма, показанная выше;
# мы включаем поток логических правил в виде «признак, значение которого меньше x, а значение признака 
# b меньше, чем y… => Категория 1» в древовидную структуру данных. Преимущество этого алгоритма в том, 
# что они легко интерпретируемы. Например, используя приведенную выше схему, банк может объяснить клиенту, 
# почему ему было отказано в ссуде: например, клиент не владеет домом, а его доход составляет менее 5000.

# Как мы увидим позже, многие другие модели, хотя и более точные, не обладают этим свойством и могут 
# рассматриваться как подход «черного ящика», когда сложнее интерпретировать, как входные данные были 
# преобразованы в выходные. Благодаря этой «понятности» и сходству с человеческим процессом принятия решений 
# (вы можете легко объяснить свою модель своему боссу) деревья решений приобрели огромную популярность. C4.5, 
# представитель этой группы методов классификации, даже занимает первое место в списке 10 лучших алгоритмов 
# интеллектуального анализа данных («10 лучших алгоритмов интеллектуального анализа данных», Knowledge and 
# Information Systems, 2008. PDF).

								# Как построить дерево решений

# Ранее мы видели, что решение о предоставлении ссуды принимается на основании возраста, активов, 
# дохода и других переменных. Но на какую переменную смотреть в первую очередь? Давайте обсудим простой 
# пример, когда все переменные являются двоичными.

# Вспомните игру «20 вопросов», на которую часто ссылаются при представлении деревьев решений. Вы, 
# наверное, играли в эту игру - один человек думает о знаменитости, а другой пытается угадать, задавая 
# только вопросы «да» или «нет». Какой вопрос задаст угадывающий в первую очередь? Конечно, они спросят 
# тот, который больше всего сужает количество оставшихся вариантов. На вопрос: «Это Анджелина Джоли?» в 
# случае отрицательного ответа оставит в поле зрения всех знаменитостей, кроме одной. Напротив, вопрос: 
# «Знаменитость - женщина?» уменьшит возможности примерно до половины. То есть функция «пол» отделяет набор 
# данных о знаменитостях намного лучше, чем другие функции, такие как «Анджелина Джоли», «Испанец» или «любит 
# футбол». Это рассуждение соответствует концепции получения информации на основе энтропии.

										# Энтропия

# Энтропия Шеннона определяется для системы с N возможными состояниями следующим образом:

# где Pi - вероятность нахождения системы в i-м состоянии. Это очень важное понятие, используемое в физике, 
# теории информации и других областях. Энтропию можно описать как степень хаоса в системе. Чем выше энтропия, 
# тем менее упорядочена система и наоборот. Это поможет нам формализовать «эффективное разделение данных», о 
# котором мы упоминали в контексте «20 вопросов».

									# Пример игрушки

# Чтобы проиллюстрировать, как энтропия может помочь нам определить полезные функции для построения дерева 
# решений, давайте рассмотрим игрушечный пример. Мы спрогнозируем цвет шара в зависимости от его положения.

# Есть 9 синих шаров и 11 желтых шаров. Если случайно вытащить шар, то он будет синим с вероятностью p1 = 9/20 
# и желтым с вероятностью p2 = 11/20, что дает нам энтропию S0 = -9/20 log2 (9/20) - 11 / 20 log2 (11/20) ≈ 1. 
# Это значение само по себе может мало сказать нам, но давайте посмотрим, как изменится значение, если мы 
# разделим шары на две группы: с положением меньше или равным 12 и больше чем 12.

# В левой группе 13 шаров, 8 синих и 5 желтых. Энтропия этой группы S1 = -5/13 log2 (5/13) - 8/13 log2 (8/13) ≈ 0,96. 
# В правой группе 7 шаров, 1 синий и 6 желтых. Энтропия правой группы равна S2 = -1/7 log2 (1/7) - 6/7 log2 (6/7) ≈ 0,6. 
# Как видите, энтропия уменьшилась в обеих группах, особенно в правой. Поскольку энтропия, по сути, является степенью 
# хаоса (или неопределенности) в системе, уменьшение энтропии называется приростом информации. Формально информационный 
# прирост (IG) для разделения на основе переменной Q (в этом примере это переменная «x ≤ 12») определяется как

# где q - количество групп после разделения, Ni - количество объектов из выборки, в которой переменная Q равна i-му значению. 
# В нашем примере наше разделение дало две группы (q = 2), одна с 13 элементами (N1 = 13), другая с 7 (N2 = 7). Следовательно, 
# мы можем вычислить прирост информации как

# Оказывается, разделение шаров на две группы по принципу «координата меньше или равна 12» дало нам более упорядоченную систему. 
# Давайте продолжим делить их на группы, пока все шары в каждой группе не станут одного цвета.

# Для правой группы мы можем легко увидеть, что нам нужен только один дополнительный раздел, используя «координату, 
# меньшую или равную 18». Но для левой группы нам нужно еще три. Обратите внимание, что энтропия группы, в которой 
# все шары одного цвета, равна 0 (log2 (1) = 0).

# Мы успешно построили дерево решений, которое предсказывает цвет шара в зависимости от его положения. 
# Это дерево решений может не сработать, если мы добавим какие-либо шары, потому что оно идеально подходит 
# для обучающего набора (начальные 20 мячей). Если бы мы хотели преуспеть в этом случае, дерево с меньшим 
# количеством «вопросов» или разделений было бы более точным, даже если оно не идеально подходит для 
# обучающей выборки. Мы обсудим проблему переоснащения позже.

# Алгоритм построения дерева
# Мы можем убедиться, что дерево, построенное в предыдущем примере, является оптимальным: потребовалось 
# всего 5 «вопросов» (обусловленных переменной x), чтобы идеально подогнать дерево решений к обучающей 
# выборке. При других условиях разделения результирующее дерево будет глубже, т.е. потребуется больше 
# «вопросов», чтобы получить ответ.

# В основе популярных алгоритмов построения дерева решений, таких как ID3 или C4.5, лежит принцип жадной 
# максимизации прироста информации: на каждом шаге алгоритм выбирает переменную, которая дает наибольший 
# прирост информации при расщеплении. Затем процедура повторяется рекурсивно до тех пор, пока энтропия не 
# станет равной нулю (или некоторому небольшому значению для учета переобучения). Различные алгоритмы используют 
# разные эвристики для «ранней остановки» или «отсечения», чтобы избежать построения переобученного дерева.


# Другие критерии качества для разделения в задачах классификации
# Мы обсудили, как энтропия позволяет формализовать разделы в дереве. Но это только одна эвристика; есть другие.

# Неопределенность Джини (примесь Джини)
# Максимальное увеличение этого критерия можно интерпретировать как максимизацию количества пар объектов 
# одного класса, находящихся в одном поддереве (не путать с индексом Джини)

# Ошибка неправильной классификации
# На практике ошибка неправильной классификации почти никогда не используется, и неопределенность Джини 
# и получение информации работают аналогично.

# Для бинарной классификации энтропия и неопределенность Джини принимают следующий вид:
# где (p + - вероятность наличия у объекта метки +).

# Если мы построим график этих двух функций против аргумента p +, мы увидим, что график энтропии очень 
# близок к графику неопределенности Джини, удвоенной. Поэтому на практике эти два критерия практически идентичны.


					# Пример

# Давайте рассмотрим подгонку дерева решений к некоторым синтетическим данным. Мы сгенерируем образцы из двух 
# классов, оба с нормальным распределением, но с разными средствами.

# # first class
# np.random.seed(17)
# train_data = np.random.normal(size=(100, 2))
# train_labels = np.zeros(100)
# # adding second class
# train_data = np.r_[train_data, np.random.normal(size=(100, 2), loc=2)]
# train_labels = np.r_[train_labels, np.ones(100)]

# Построим данные. Неформально проблема классификации в этом случае состоит в том, чтобы построить некоторую «хорошую»
# границу, разделяющую два класса (красные точки от желтого). Машинное обучение в этом случае сводится к выбору хорошей
# разделяющей границы. Прямая линия будет слишком простой, в то время как некоторые сложные кривые, извивающиеся каждой
# красной точкой, будут слишком сложными и приведут нас к ошибкам на новых образцах. Интуитивно понятно, что некоторая 
# гладкая граница или, по крайней мере, прямая линия или гиперплоскость хорошо подойдет для новых данных.

# plt.rcParams['figure.figsize'] = (10,8)
# plt.scatter(train_data[:, 0], train_data[:, 1], c=train_labels, s=100, 
# cmap='autumn', edgecolors='black', linewidth=1.5);
# plt.plot(range(-2,5), range(4,-3,-1));

# Давайте попробуем разделить эти два класса, обучив дерево решений Sklearn. Мы будем использовать параметр max_depth, 
# который ограничивает глубину дерева. Визуализируем получившуюся разделительную границу.


from sklearn.tree import DecisionTreeClassifier

# Напишем вспомогательную функцию, которая будет возвращать сетку для дальнейшей визуализации.
def get_grid(data):
    x_min, x_max = data[:, 0].min() - 1, data[:, 0].max() + 1
    y_min, y_max = data[:, 1].min() - 1, data[:, 1].max() + 1
    return np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))

# clf_tree = DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=17)

# обучение дерева
# clf_tree.fit(train_data, train_labels)

# код для изображения разделяющей поверхности
# xx, yy = get_grid(train_data)
# predicted = clf_tree.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)

# plt.subplots(1, 1, sharey = True, figsize = (10, 7)) 

# plt.pcolormesh(xx, yy, predicted, cmap='autumn')
# plt.scatter(train_data[:, 0], train_data[:, 1], c=train_labels, s=100, 
# cmap='autumn', edgecolors='black', linewidth=1.5);

# А как выглядит само дерево? Мы видим, что дерево «разрезает» пространство на 8 прямоугольников, т.е. у 
# дерева 8 листьев. Внутри каждого прямоугольника дерево будет делать прогноз в соответствии с меткой 
# большинства объектов внутри него.

# использовать формат .dot для визуализации дерева
from ipywidgets import Image
from io import StringIO
import pydotplus 
from sklearn.tree import export_graphviz
# dot_data = StringIO()
# export_graphviz(clf_tree, feature_names=['x1', 'x2'], out_file=dot_data, filled=True)
# graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  
# Image(value = graph.create_png())																	# ???
# graph.write_pdf("Tree_Topic/Tree_1.pdf")

# Как мы можем «прочитать» такое дерево?
# Вначале было 200 образцов (экземпляров, samples), по 100 каждого класса. Энтропия исходного состояния 
# была максимальной, S = 1. Затем было выполнено первое разделение выборок на 2 группы путем сравнения 
# значения x2 с 1,211 (найдите эту часть границы на рисунке выше). При этом энтропия как левой, так и 
# правой группы уменьшилась. Процесс продолжается до глубины 3. В этой визуализации чем больше выборок 
# первого класса, тем темнее оранжевый цвет вершины; чем больше образцов второго класса, тем темнее синий 
# цвет. Вначале количество выборок из двух классов одинаково, поэтому корневой узел дерева белый.


			# Как дерево решений работает с числовыми характеристиками

# Предположим, у нас есть числовая функция «Возраст», которая имеет множество уникальных значений. 
# Дерево решений будет искать лучшее (в соответствии с некоторым критерием получения информации) разбиение, 
# проверяя двоичные атрибуты, такие как «Возраст <17», «Возраст <22,87» и т. Д. Но что, если возрастной 
# диапазон большой? Или что, если еще одна количественная переменная, «зарплата», тоже может быть «урезана» 
# разными способами? Будет слишком много двоичных атрибутов для выбора на каждом этапе построения дерева. 
# Чтобы решить эту проблему, обычно используются эвристики для ограничения количества пороговых значений, 
# с которыми мы сравниваем количественную переменную.

# Рассмотрим пример. Предположим, у нас есть следующий набор данных:
# data = pd.DataFrame({'Age': [17,64,18,20,38,49,55,25,29,31,33], 'Loan Default': [1,0,1,0,1,0,0,1,1,0,1]})
# # Отсортируем по возрасту в порядке возрастания.
# # print(data.head(20))
# data.sort_values('Age')
# # print(data.sort_values('Age').head(20))

# age_tree = DecisionTreeClassifier(random_state=17)
# age_tree.fit(data['Age'].values.reshape(-1, 1), data['Loan Default'].values)
# dot_data = StringIO()
# export_graphviz(age_tree, feature_names=['Age'], out_file=dot_data, filled=True)
# graph = pydotplus.graph_from_dot_data(dot_data.getvalue())
# Image(value=graph.create_png())
# graph.write_pdf("Tree_Topic/Tree_2.pdf")

# Мы видим, что дерево использовало следующие 5 значений для оценки по возрасту: 43,5, 19, 22,5, 30 и 
# 32 года. Если вы присмотритесь, это именно средние значения между возрастами, в которых целевой класс 
# «переключается» с 1 на 0 или с 0 на 1. Чтобы проиллюстрировать далее, 43,5 - это среднее значение для 
# 38 и 49 лет; 38-летний клиент не вернул ссуду, тогда как 49-летний клиент вернул. Дерево ищет значения, 
# при которых целевой класс переключает свое значение в качестве порога для «отсечения» количественной 
# переменной.

# Учитывая эту информацию, почему вы думаете, что здесь нет смысла рассматривать такую функцию, как «Возраст <17,5»?
# Давайте рассмотрим более сложный пример, добавив переменную «Зарплата» (в тысячах долларов в год).

# data2 = pd.DataFrame({'Age':  [17,64,18,20,38,49,55,25,29,31,33],
#                       'Salary': [25,80,22,36,37,59,74,70,33,102,88],
#                       'Loan Default': [1,0,1,0,1,0,0,1,1,0,1]})
# data2.sort_values('Age')
# print(data2.sort_values('Age').head(20))

# Если отсортировать по возрасту, целевой класс («невыполнение кредита») переключается (с 1 на 0 или наоборот) 5 раз. 
# А если сортировать по зарплате, переключается 7 раз. Как дерево теперь будет выбирать объекты? Давайте посмотрим.

# age_sal_tree = DecisionTreeClassifier(random_state=17)
# age_sal_tree.fit(data2[['Age', 'Salary']].values, data2['Loan Default'].values)
# dot_data = StringIO()
# export_graphviz(age_sal_tree, feature_names=['Age', 'Salary'], 
#                 out_file=dot_data, filled=True)
# graph = pydotplus.graph_from_dot_data(dot_data.getvalue())
# Image(value=graph.create_png())
# graph.write_pdf("Tree_Topic/Tree_3.pdf")

# Мы видим, что дерево разделено как по заработной плате, так и по возрасту. Более того, пороговые значения для 
# сравнения характеристик составляют 43,5 и 22,5 года и 95 тысяч и 30,5 тысяч в год. Опять же, мы видим, что 95 
# - это среднее значение между 88 и 102; человек с зарплатой 88 тыс. оказался «плохим», а человек с 102 тыс. - «хорошо». 
# То же самое и с 30,5к. То есть искали только несколько значений для сравнения по возрасту и заработной плате. Почему
# дерево выбрало эти особенности? Потому что они дали лучшее разделение (согласно неопределенности Джини).

# Вывод: простейшая эвристика для обработки числовых признаков в дереве решений - это отсортировать его значения в 
# порядке возрастания и проверить только те пороговые значения, в которых изменяется значение целевой переменной.
# Кроме того, когда в наборе данных много числовых функций, каждая из которых имеет множество уникальных значений, 
# выбираются только верхние N пороговых значений, описанных выше, то есть используются только верхние N, которые 
# дают максимальный выигрыш. Процесс состоит в построении дерева глубины 1, вычислении энтропии (или неопределенности 
# Джини) и выборе лучших пороговых значений для сравнения.

# Чтобы проиллюстрировать, если мы разделим на «Зарплата ≤ 34,5», левая подгруппа будет иметь энтропию 0 (все клиенты 
# «плохие»), а правая - 0,954 (3 «плохих» и 5 «хороших»). », Вы можете проверить это сами, так как это будет частью 
# задания). Прирост информации составляет примерно 0,3. Если мы разделим на «Зарплата ≤ 95», левая подгруппа будет 
# иметь энтропию 0,97 (6 «плохо» и 4 «хорошо»), а правая будет иметь энтропию 0 (группа, содержащая только один объект). 
# Прирост информации составляет около 0,11. Если мы таким образом вычислим прирост информации для каждого раздела, мы 
# сможем выбрать пороговые значения для сравнения каждой числовой характеристики перед построением большого дерева 
# (с использованием всех функций).

# Больше примеров дискретизации числовых функций можно найти в таких сообщениях. Одна из наиболее известных научных 
# работ по этой теме - «Об обработке непрерывнозначных атрибутов при генерации дерева решений» (У. М. Файяд. КБ Ирани, 
# «Машинное обучение», 1992).

									# Ключевые параметры дерева

# Технически вы можете построить дерево решений до тех пор, пока каждый лист не будет иметь ровно один экземпляр, 
# но на практике это не является обычным при построении одного дерева, потому что оно будет переобучено или слишком 
# настроено на обучающий набор и не будет предсказывать метки для новых данных. хорошо. Внизу дерева, на некоторой 
# большой глубине, будут разделены менее важные функции (например, был ли клиент из Лидса или Нью-Йорка). Мы можем 
# еще больше преувеличить эту историю и обнаружить, что все четыре клиента, пришедшие в банк за ссудой в зеленых штанах, 
# не вернули ссуду. Даже если бы это было верно при обучении, мы не хотим, чтобы наша модель классификации генерировала 
# такие конкретные правила.

# Есть два исключения, когда деревья построены на максимальную глубину:

# Случайный лес (группа деревьев) усредняет ответы отдельных деревьев, построенных на максимальной 
# глубине (позже мы поговорим о том, почему вы должны это делать)

# Обрезка деревьев. При таком подходе дерево сначала строится на максимальную глубину. Затем снизу вверх удаляются 
# некоторые узлы дерева путем сравнения качества дерева с этим разделом и без него (сравнение выполняется с 
# использованием перекрестной проверки, подробнее об этом ниже).

# На рисунке ниже показан пример разделительной границы, построенной в переобученном дереве.

# Наиболее распространенные способы справиться с переобучением в деревьях решений следующие:
# 1) искусственное ограничение глубины или минимального количества выборок в листьях: строительство дерева 
# просто останавливается в какой-то момент;
# 2) обрезка дерева.

							# Class DecisionTreeClassifier in Scikit-learn

# Основные параметры класса sklearn.tree.DecisionTreeClassifier:
# max_depth - максимальная глубина дерева;
# max_features - максимальное количество функций для поиска лучшего раздела (это необходимо при большом 
# количестве функций, потому что было бы «дорого» искать разделы для всех функций);
# min_samples_leaf - минимальное количество выборок в листе. Этот параметр предотвращает создание деревьев, 
# в которых любой лист будет иметь только несколько элементов.

# Параметры дерева должны быть установлены в зависимости от входных данных, и обычно это делается 
# посредством перекрестной проверки (cross-validation), подробнее об этом ниже.

									# Дерево решений в задаче регрессии

# При прогнозировании числовой переменной идея построения дерева остается прежней, но критерии качества меняются.

# где n - количество выборок в листе, Yi - значение целевой переменной. Проще говоря, минимизируя дисперсию вокруг 
# среднего, мы ищем функции, которые делят обучающий набор таким образом, что значения целевой функции в каждом 
# листе примерно равны.

# Давайте сгенерируем данные, распределенные функцией с некоторым шумом. 
# Затем мы обучим на нем дерево и покажем, какие прогнозы оно дает.

# n_train = 150        
# n_test = 1000       
# noise = 0.1
# def f(x):
#     x = x.ravel()
#     return np.exp(-x ** 2) + 1.5 * np.exp(-(x - 2) ** 2)
# def generate(n_samples, noise):
#     X = np.random.rand(n_samples) * 10 - 5
#     X = np.sort(X).ravel()
#     y = np.exp(-X ** 2) + 1.5 * np.exp(-(X - 2) ** 2) + \
#     np.random.normal(0.0, noise, n_samples)
#     X = X.reshape((n_samples, 1))
#     return X, y
# X_train, y_train = generate(n_samples=n_train, noise=noise)
# X_test, y_test = generate(n_samples=n_test, noise=noise)
# from sklearn.tree import DecisionTreeRegressor
# reg_tree = DecisionTreeRegressor(max_depth=5, random_state=17)
# reg_tree.fit(X_train, y_train)
# reg_tree_pred = reg_tree.predict(X_test)
# plt.figure(figsize=(10, 6))
# plt.plot(X_test, f(X_test), "b")
# plt.scatter(X_train, y_train, c="b", s=20)
# plt.plot(X_test, reg_tree_pred, "g", lw=2)
# plt.xlim([-5, 5])
# plt.title("Decision tree regressor, MSE = %.2f" % np.sum((y_test - reg_tree_pred) ** 2))

# Мы видим, что дерево решений аппроксимирует данные кусочно-постоянной функцией.

						# 3. Метод ближайших соседей

# Метод ближайших соседей (k-Nearest Neighbors, или k-NN) - еще один очень популярный метод классификации, 
# который также иногда используется в задачах регрессии. Это, как и деревья решений, один из наиболее 
# понятных подходов к классификации. Основная интуиция заключается в том, что вы похожи на своих соседей. 
# Более формально метод следует гипотезе компактности: если расстояние между примерами измерено достаточно 
# хорошо, то похожие примеры с гораздо большей вероятностью принадлежат к одному классу.

# Согласно методу ближайших соседей, зеленый шар будет классифицироваться как «синий», а не «красный».

# Другой пример: если вы не знаете, как пометить Bluetooth-гарнитуру в онлайн-списке, вы можете найти 5 
# похожих гарнитур, и, если 4 из них помечены как «аксессуары» и только 1 как «Технологии», то вы он также 
# будет помечен как «аксессуары».

# Чтобы классифицировать каждый образец из набора тестов, необходимо выполнить следующие операции по порядку:
# 1) Рассчитайте расстояние до каждого образца в обучающей выборке.
# 2) Выберите k образцов из обучающей выборки с минимальным расстоянием до них.
# 3) Класс тестовой выборки будет самым частым классом среди этих k ближайших соседей.

# Метод довольно легко адаптируется к задаче регрессии: на шаге 3 он возвращает не класс, а число - среднее 
# (или медиану) целевой переменной среди соседей.

# Примечательной особенностью этого подхода является его ленивость - вычисления выполняются только на этапе 
# прогнозирования, когда необходимо классифицировать тестовую выборку. Предварительно из обучающих примеров 
# не строится никакой модели. Напротив, напомним, что для деревьев решений в первой половине этой статьи 
# дерево строится на основе обучающего набора, и классификация тестовых примеров происходит относительно 
# быстро путем обхода дерева.

# Ближайшие соседи - это хорошо проработанный подход. Существует множество важных теорем, утверждающих, 
# что для «бесконечных» наборов данных это оптимальный метод классификации. Авторы классической книги 
# «Элементы статистического обучения» считают k-NN теоретически идеальным алгоритмом, использование которого 
# ограничено только вычислительной мощностью и проклятием размерности.

					# Метод ближайших соседей в реальных приложениях

# k-NN может служить хорошей отправной точкой (базой) в некоторых случаях;
# В соревнованиях Kaggle k-NN часто используется для построения мета-характеристик (т.е. предсказаний k-NN 
# в качестве входных данных для других моделей) или для суммирования / смешивания;
# Метод ближайших соседей распространяется на другие задачи, такие как системы рекомендаций. Первоначальным 
# решением может быть рекомендация продукта (или услуги), популярного среди ближайших соседей человека, 
# которому мы хотим порекомендовать;
# На практике на больших наборах данных часто используются приближенные методы поиска ближайших соседей. 
# Существует ряд библиотек с открытым исходным кодом, реализующих такие алгоритмы; посетите библиотеку Spotify Annoy.
# https://github.com/spotify/annoy

# Качество классификации / регрессии с k-NN зависит от нескольких параметров:
# 1) Количество соседей k.
# 2) Измерение расстояния между выборками (обычно это расстояния Хэмминга, Евклида, косинуса и Минковского). 
# Обратите внимание, что для большинства этих показателей требуется масштабирование данных. Проще говоря, 
# мы не хотим, чтобы функция «зарплата», которая составляет порядка тысяч, влияла на расстояние больше, 
# чем на «возраст», который обычно меньше 100.
# 3) Веса соседей (каждый сосед может вносить разные веса; например, чем дальше образец, тем меньше вес).


					# Класс KNeighborsClassifier в Scikit-learn

# Основными параметрами класса sklearn.neighbors.KNeighborsClassifier являются:
# 1) веса: однородные (все веса равны), расстояние (вес обратно пропорционален расстоянию от 
# тестового образца) или любая другая определяемая пользователем функция;
# 2) алгоритм (необязательно): brute, ball_tree, KD_tree или auto. В первом случае ближайшие 
# соседи для каждого тестового примера вычисляются путем поиска по сетке по обучающему набору. 
# Во втором и третьем случаях расстояния между примерами хранятся в дереве, чтобы ускорить поиск 
# ближайших соседей. Если вы установите для этого параметра значение auto, правильный способ поиска 
# соседей будет выбран автоматически на основе обучающего набора.
# 3) leaf_size (необязательно): порог перехода к поиску по сетке, если алгоритм поиска 
# соседей BallTree или KDTree;
# 4) метрика: минковский, манхэттенский, евклидовый, чебышевский и др.

				# 4. Выбор параметров модели и перекрестная проверка (Cross-Validation)

# Основная задача алгоритмов обучения - уметь обобщать невидимые данные. Поскольку мы не можем 
# сразу проверить производительность модели на новых, входящих данных (потому что мы еще не знаем 
# истинные значения целевой переменной), необходимо пожертвовать небольшой частью данных, чтобы 
# проверить качество модели на ней.

# Часто это делается одним из двух способов:
# 1) зарезервировать часть набора данных (удерживаемый / удерживаемый набор). Мы резервируем часть 
# обучающего набора (обычно от 20% до 40%), обучаем модель на оставшихся данных (60–80% исходного набора) 
# и вычисляем показатели производительности для модели (например, точность) на удержании. -выходной набор.
# 2) перекрестная проверка. Наиболее частым случаем здесь является k-кратная перекрестная проверка.

# При k-кратной перекрестной проверке модель обучается K раз на разных (K-1) подмножествах исходного набора 
# данных (выделено белым цветом) и проверяется на оставшемся подмножестве (каждый раз на другом подмножестве, 
# показанном выше оранжевым цветом). Мы получаем K оценок качества модели, которые обычно усредняются, 
# чтобы дать общее среднее качество классификации / регрессии.

# Перекрестная проверка обеспечивает лучшую оценку качества модели на новых данных по сравнению с подходом 
# с удерживаемым набором. Однако перекрестная проверка требует больших вычислительных ресурсов, 
# когда у вас много данных.
# Перекрестная проверка - очень важный метод машинного обучения, который также может применяться в 
# статистике и эконометрике. Он помогает с настройкой гиперпараметров, сравнением моделей, оценкой 
# характеристик и т. Д. Более подробную информацию можно найти здесь (сообщение в блоге Себастьяна Рашки) 
# или в любом классическом учебнике по машинному (статистическому) обучению.
# https://sebastianraschka.com/blog/2016/model-evaluation-selection-part1.html

					 # 5. Примеры применения и сложные случаи

# Метод деревьев решений и ближайших соседей в задаче прогнозирования оттока клиентов
# Давайте прочитаем данные в DataFrame и обработаем их. На данный момент сохраните состояние 
# в отдельном объекте Series и удалите его из фрейма данных. Мы обучим первую модель без 
# функции State, а затем посмотрим, поможет ли она.
pd.set_option('display.max_columns', 100)
df = pd.read_csv('../mlcourse.ai/data/telecom_churn.csv')
df['International plan'] = pd.factorize(df['International plan'])[0]
df['Voice mail plan'] = pd.factorize(df['Voice mail plan'])[0]
df['Churn'] = df['Churn'].astype('int')
states = df['State']
y = df['Churn']
df.drop(['State', 'Churn'], axis=1, inplace=True)

print(df.head(10))

# Давайте выделим 70% набора для обучения (X_train, y_train) и 30% для набора удержания (X_holdout, y_holdout). 
# Комплект удержания не будет задействован в настройке параметров моделей. Воспользуемся им в конце, после 
# настройки, чтобы оценить качество полученной модели. Обучим 2 модели: дерево решений и k-NN. Мы не знаем, 
# какие параметры являются хорошими, поэтому предположим несколько случайных: глубина дерева 5 и количество 
# ближайших соседей равно 10.

from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.neighbors import KNeighborsClassifier

X_train, X_holdout, y_train, y_holdout = train_test_split(df.values, y, test_size=0.3, random_state=17)
tree = DecisionTreeClassifier(max_depth=5, random_state=17)
knn = KNeighborsClassifier(n_neighbors=10)
tree.fit(X_train, y_train)
knn.fit(X_train, y_train)

# Давайте оценим качество прогнозов на нашем удерживаемом наборе с помощью простого показателя - 
# доли правильных ответов (точности). Дерево решений сработало лучше - процент правильных ответов 
# составляет около 94% (дерево решений) против 88% (k-NN). Обратите внимание, что эта производительность 
# достигается за счет использования случайных параметров.

from sklearn.metrics import accuracy_score

tree_pred = tree.predict(X_holdout)
print(accuracy_score(y_holdout, tree_pred)) # 0.94
knn_pred = knn.predict(X_holdout)
print(accuracy_score(y_holdout, knn_pred)) # 0.88


# Теперь давайте определим параметры дерева с помощью перекрестной проверки. Мы настроим максимальную 
# глубину и максимальное количество функций, используемых для каждого разделения. Вот суть того, 
# как работает GridSearchCV: для каждой уникальной пары значений max_depth и max_features вычислите 
# производительность модели с 5-кратной перекрестной проверкой, а затем выберите лучшую комбинацию параметров.

from sklearn.model_selection import GridSearchCV, cross_val_score

# tree_params = {'max_depth': range(1,11),'max_features': range(4,19)}

# tree_grid = GridSearchCV(tree, tree_params, cv=5, n_jobs=-1, verbose=True)

# tree_grid.fit(X_train, y_train)

# Перечислим лучшие параметры и соответствующую среднюю точность перекрестной проверки.

# print(tree_grid.best_params_) # {'max_depth': 6, 'max_features': 17}
# print(tree_grid.best_score_) # 0.942
# print(accuracy_score(y_holdout, tree_grid.predict(X_holdout))) # 0.946

# Нарисуем получившееся дерево. Из-за того, что это не совсем игрушечный пример (максимальная глубина 6), 
# картинка не такая уж и маленькая, но вы можете «пройтись» по дереву, если локально откроете соответствующее 
# изображение, загруженное из репозитория курса.

# dot_data = StringIO()
# export_graphviz(tree_grid.best_estimator_, feature_names=df.columns, out_file=dot_data, filled=True)
# graph = pydotplus.graph_from_dot_data(dot_data.getvalue())
# Image(value=graph.create_png())
# graph.write_pdf("Tree_Topic/Tree_4.pdf")

# Теперь давайте настроим количество соседей k для k-NN:
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
knn_pipe = Pipeline([('scaler', StandardScaler()),('knn', KNeighborsClassifier(n_jobs=-1))])
knn_params = {'knn__n_neighbors': range(1, 10)}
knn_grid = GridSearchCV(knn_pipe, knn_params, cv=5, n_jobs=-1, verbose=True)
knn_grid.fit(X_train, y_train)
print(knn_grid.best_params_, knn_grid.best_score_)
({'knn__n_neighbors': 7}, 0.886)

# Здесь дерево оказалось лучше, чем алгоритм ближайших соседей: точность 94,2% / 94,6% для перекрестной 
# проверки и удержания соответственно. Деревья решений работают очень хорошо, и даже случайный лес (давайте 
# сейчас представим его как группу деревьев, которые лучше работают вместе) в этом примере не может обеспечить 
# лучшую производительность (95,1% / 95,3%), несмотря на то, что обучение длилось намного дольше.

from sklearn.ensemble import RandomForestClassifier

# forest = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=17)
# print(np.mean(cross_val_score(forest, X_train, y_train, cv=5))) # 0.949

# forest_params = {'max_depth': range(1, 11), 'max_features': range(4, 19)}

# forest_grid = GridSearchCV(forest, forest_params, cv=5, n_jobs=-1, verbose=True)

# forest_grid.fit(X_train, y_train)

# print(forest_grid.best_params_, forest_grid.best_score_) 
# ({'max_depth': 9, 'max_features': 6}, 0.951)

										# Сложный случай для деревьев решений

# Чтобы продолжить обсуждение плюсов и минусов рассматриваемых методов, давайте рассмотрим простую 
# задачу классификации, в которой дерево будет работать хорошо, но делает это «чрезмерно сложным» способом. 
# Давайте создадим набор точек на плоскости (2 объекта), каждая точка будет одного из двух классов (+1 для 
# красного или -1 для желтого). Если вы посмотрите на это как на проблему классификации, это покажется 
# очень простым: классы разделены линией.

def form_linearly_separable_data(n=500, x1_min=0, x1_max=30, 
                                 x2_min=0, x2_max=30):
    data, target = [], []
    for i in range(n):
        x1 = np.random.randint(x1_min, x1_max)
        x2 = np.random.randint(x2_min, x2_max)
        if np.abs(x1 - x2) > 0.5:
            data.append([x1, x2])
            target.append(np.sign(x1 - x2))
    return np.array(data), np.array(target)

X, y = form_linearly_separable_data()

plt.scatter(X[:, 0], X[:, 1], c=y, cmap='autumn', edgecolors='black');

# Однако граница, которую строит дерево решений, слишком сложна; плюс само дерево очень глубокое. Кроме того, 
# представьте, насколько плохо дерево будет распространяться на пространство за пределами квадратов 30 x 30, 
# которые обрамляют обучающий набор.


tree = DecisionTreeClassifier(random_state=17).fit(X, y)
xx, yy = get_grid(X)
predicted = tree.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)

plt.subplots(1, 1, sharey = True, figsize = (10, 7)) 

plt.pcolormesh(xx, yy, predicted, cmap='autumn')
plt.scatter(X[:, 0], X[:, 1], c=y, s=100, cmap='autumn', edgecolors='black', linewidth=1.5)
plt.title('Easy task. Decision tree complexifies everything');

# Мы получили эту слишком сложную конструкцию, хотя решение представляет собой прямую x1 = x2

dot_data = StringIO()
export_graphviz(tree, feature_names=['x1', 'x2'], 
                out_file=dot_data, filled=True)
graph = pydotplus.graph_from_dot_data(dot_data.getvalue())
Image(value=graph.create_png())

graph.write_pdf("Tree_Topic/Tree_5.pdf")

# Метод одного ближайшего соседа работает лучше, чем дерево, но все же не так хорош, как линейный классификатор (наша следующая тема).

knn = KNeighborsClassifier(n_neighbors=1).fit(X, y)
xx, yy = get_grid(X)
predicted = knn.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)

plt.subplots(1, 1, sharey = True, figsize = (10, 7)) 

plt.pcolormesh(xx, yy, predicted, cmap='autumn')
plt.scatter(X[:, 0], X[:, 1], c=y, s=100, 
cmap='autumn', edgecolors='black', linewidth=1.5);
plt.title('Easy task, kNN. Not bad');

			# Деревья решений и k-NN в задаче распознавания рукописных цифр MNIST

# Теперь давайте посмотрим, как эти два алгоритма работают с реальной задачей. Мы будем использовать 
# встроенный набор данных sklearn для рукописных цифр. Эта задача является примером того, как k-NN 
# работает на удивление хорошо.
# Картинки здесь представляют собой матрицы 8х8 (интенсивность белого цвета для каждого пикселя). 
# Затем каждая такая матрица «разворачивается» в вектор длиной 64, и мы получаем характеристическое 
# описание объекта.

# Давайте нарисуем рукописные цифры. Мы видим, что они различимы.

from sklearn.datasets import load_digits
# data = load_digits()
# X, y = data.data, data.target
# f, axes = plt.subplots(1, 4, sharey=True, figsize=(16, 6)) 
# for i in range(4): 
#     axes[i].imshow(X[i,:].reshape([8,8]), cmap='Greys');

# Затем давайте проведем тот же эксперимент, что и в предыдущем задании, но на этот раз изменим диапазоны настраиваемых параметров.
# Давайте выберем 70% набора данных для обучения (X_train, y_train) и 30% для удержания (X_holdout, y_holdout). Набор удерживающих 
# устройств не будет участвовать в настройке параметров модели; мы воспользуемся им в конце, чтобы проверить качество полученной модели.

# X_train, X_holdout, y_train, y_holdout = train_test_split(X, y, test_size=0.3, random_state=17)

# Давайте обучим дерево решений и k-NN с нашими случайными параметрами и сделаем прогнозы на нашем удерживающем множестве.
# Мы видим, что k-NN работает намного лучше, но обратите внимание, что это со случайными параметрами.

tree = DecisionTreeClassifier(max_depth=5, random_state=17)
knn = KNeighborsClassifier(n_neighbors=10)
tree.fit(X_train, y_train)
knn.fit(X_train, y_train)

tree_pred = tree.predict(X_holdout)
knn_pred = knn.predict(X_holdout)
print(accuracy_score(y_holdout, knn_pred), accuracy_score(y_holdout, tree_pred)) 
(0.97, 0.666)

# Теперь давайте настроим параметры нашей модели, используя перекрестную проверку, как и раньше, но теперь мы учтем, 
# что у нас больше функций, чем в предыдущей задаче: 64.

# tree_params = {'max_depth': [1, 2, 3, 5, 10, 20, 25, 30, 40, 50, 64], 'max_features': [1, 2, 3, 5, 10, 20 ,30, 50, 64]}
# tree_grid = GridSearchCV(tree, tree_params, cv=5, n_jobs=-1, verbose=True)
# tree_grid.fit(X_train, y_train)

# Давайте посмотрим на лучшую комбинацию параметров и соответствующую точность перекрестной проверки:

# print(tree_grid.best_params_, tree_grid.best_score_) 
# ({'max_depth': 20, 'max_features': 64}, 0.844)

# Это уже превышает 66%, но не совсем 97%. k-NN лучше работает с этим набором данных. В случае одного ближайшего 
# соседа мы смогли достичь 99% предположений при перекрестной проверке.
# print(np.mean(cross_val_score(KNeighborsClassifier(n_neighbors=1), X_train, y_train, cv=5))) # 0.987

# Давайте обучим случайный лес на том же наборе данных, он работает лучше, чем k-NN для большинства наборов данных.
# Но здесь есть исключение.
# print(np.mean(cross_val_score(RandomForestClassifier(random_state=17), X_train, y_train, cv=5))) # 0.935

# Вы правильно заметили, что мы не настраивали здесь какие-либо параметры RandomForestClassifier. Даже с настройкой 
# точность обучения не достигает 98%, как это было с одним ближайшим соседом.

# Вывод этого эксперимента (и общий совет): сначала проверьте простые модели на ваших данных: дерево решений и 
# ближайшие соседи (в следующий раз мы также добавим в этот список логистическую регрессию). Возможно, эти 
# методы уже работают достаточно хорошо.

# Сложный случай для метода ближайших соседей.
# Рассмотрим еще один простой пример. В задаче классификации одна из характеристик будет просто пропорциональна 
# вектору ответов, но это не поможет для метода ближайших соседей.

def form_noisy_data(n_obj=1000, n_feat=100, random_seed=17):
    np.seed = random_seed
    y = np.random.choice([-1, 1], size=n_obj)
    # первая функция пропорциональна цели
    x1 = 0.3 * y
    # другие функции - шум
    x_other = np.random.random(size=[n_obj, n_feat - 1])
    return np.hstack([x1.reshape([n_obj, 1]), x_other]), y
X, y = form_noisy_data()

# Как всегда, мы будем смотреть на точность перекрестной проверки и набор задержек. Построим кривые, отражающие 
# зависимость этих величин от параметра n_neighbors в методе ближайших соседей. Эти кривые называются кривыми проверки.
# Можно видеть, что k-NN с евклидовым расстоянием не очень хорошо работает с задачей, даже если вы изменяете 
# количество ближайших соседей в широком диапазоне.

X_train, X_holdout, y_train, y_holdout = train_test_split(X, y, test_size=0.3, random_state=17)
from sklearn.model_selection import cross_val_score
cv_scores, holdout_scores = [], []
n_neighb = [1, 2, 3, 5] + list(range(50, 550, 50))

for k in n_neighb:
    knn = KNeighborsClassifier(n_neighbors=k)
    cv_scores.append(np.mean(cross_val_score(knn, X_train, y_train, cv=5)))
    knn.fit(X_train, y_train)
    holdout_scores.append(accuracy_score(y_holdout, knn.predict(X_holdout)))

plt.subplots(1, 1, sharey = True, figsize = (10, 7)) 

plt.plot(n_neighb, cv_scores, label='CV')
plt.plot(n_neighb, holdout_scores, label='holdout')
plt.title('Easy task. kNN fails')
plt.legend();

# В отличие от этого, дерево решений легко «обнаруживает» скрытые зависимости в данных, 
# несмотря на ограничение максимальной глубины.

tree = DecisionTreeClassifier(random_state=17, max_depth=1)
tree_cv_score = np.mean(cross_val_score(tree, X_train, y_train, cv=5))
tree.fit(X_train, y_train)
tree_holdout_score = accuracy_score(y_holdout, tree.predict(X_holdout))
print('Decision tree. CV: {}, holdout: {}'.format(tree_cv_score, tree_holdout_score)) 
# Decision tree. CV: 1.0, holdout: 1.0

# Во втором примере дерево прекрасно решало проблему, в то время как k-NN испытывал трудности. 
# Однако это скорее недостаток использования евклидова расстояния, чем метода. Это не позволило 
# нам выявить, что одна функция намного лучше других.

					# 6. Плюсы и минусы деревьев решений и метода ближайших соседей.

							# Плюсы и минусы деревьев решений
# Плюсы:
# Создание четких и понятных для человека правил классификации, например «Если возраст <25 и 
# интересуется мотоциклами, отказать в ссуде». Это свойство называется интерпретируемостью модели.
# Деревья решений можно легко визуализировать, т.е. как саму модель (дерево), так и прогноз для 
# определенного тестового объекта (путь в дереве) можно «интерпретировать».
# Быстрое обучение и прогнозирование.
# Небольшое количество параметров модели.
# Поддерживает как числовые, так и категориальные функции.
# Минусы:
# Деревья очень чувствительны к шуму во входных данных; вся модель может измениться, если обучающий 
# набор будет немного изменен (например, удалить функцию, добавить несколько объектов). Это ухудшает 
# интерпретируемость модели.
# Разделительная граница, построенная деревом решений, имеет свои ограничения - она состоит из 
# гиперплоскостей, перпендикулярных одной из осей координат, что на практике уступает по качеству 
# некоторым другим методам.
# Нам нужно избежать переобучения путем обрезки, установки минимального количества выборок в каждом 
# листе или определения максимальной глубины для дерева. Обратите внимание, что переоснащение является 
# проблемой для всех методов машинного обучения.
# Нестабильность. Небольшие изменения данных могут существенно изменить дерево решений. Эта проблема 
# решается с помощью ансамблей деревьев решений (обсуждаемых в следующий раз).
# Задача поиска оптимального дерева решений является NP-полной. На практике используются некоторые 
# эвристики, такие как жадный поиск объекта с максимальным набором информации, но он не гарантирует 
# нахождение глобального оптимального дерева.
# Трудности с поддержкой отсутствующих значений в данных. По оценке Фридмана, для поддержки пробелов 
# в данных в CART требуется около 50% кода (улучшенная версия этого алгоритма реализована в sklearn).
# Модель может только интерполировать, но не экстраполировать (то же самое верно для случайных лесов 
# и увеличения деревьев). То есть дерево решений делает постоянный прогноз для объектов, которые лежат 
# за ограничивающей рамкой, установленной обучающим набором в пространстве признаков. В нашем примере с 
# желтыми и синими шарами это будет означать, что модель дает одинаковые прогнозы для всех шаров с 
# позициями> 19 или <0.

						# Плюсы и минусы метода ближайших соседей
# Плюсы:
# Простая реализация.
# Хорошо изучен.
# Обычно этот метод является хорошим первым решением не только для классификации или регрессии, но и для рекомендаций.
# Его можно адаптировать к определенной проблеме, выбрав правильные метрики или ядро (в двух словах, 
# ядро может устанавливать операцию подобия для сложных объектов, таких как графы, сохраняя при этом 
# подход k-NN одинаковым). Кстати, Александр Дьяконов, бывший топ-1 kaggler, любит простейшие k-NN, но с 
# настроенной метрикой подобия объектов.
# Хорошая интерпретируемость. Бывают исключения: если количество соседей велико, интерпретируемость ухудшается 
# («Мы не давали ему ссуду, потому что он похож на 350 клиентов, из которых 70 плохие, а это на 12% выше 
# среднего. для набора данных »).
# Минусы:
# Метод считается быстрым по сравнению с композициями алгоритмов, но количество соседей, используемых 
# для классификации, в реальной жизни обычно велико (100–150), и в этом случае алгоритм не будет работать 
# так быстро, как дерево решений.
# Если в наборе данных много переменных, трудно найти правильные веса и определить, какие характеристики 
# не важны для классификации / регрессии.
# Зависимость от выбранной метрики расстояния между объектами. Выбор евклидова расстояния по умолчанию 
# часто необоснован. Вы можете найти хорошее решение, выполнив поиск по сетке по параметрам, но это отнимает 
# много времени для больших наборов данных.
# Теоретических способов выбора количества соседей нет - только поиск по сетке (хотя часто это верно для 
# всех гиперпараметров всех моделей). В случае небольшого числа соседей метод чувствителен к выбросам, то 
# есть склонен к переобучению.
# Как правило, из-за «проклятия размерности» он не работает, когда функций много. Профессор Педро Домингос, 
# известный член сообщества машинного обучения, говорит об этом в своей популярной статье «Несколько полезных 
# фактов о машинном обучении»; также «проклятие размерности» описано в книге «Глубокое обучение» в этой главе.


plt.show()



















